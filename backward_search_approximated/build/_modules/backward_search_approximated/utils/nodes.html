
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>backward_search_approximated.utils.nodes &#8212; IPE 0.0.1 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=d75fae25" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="../../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../../_static/documentation_options.js?v=d45e8c67"></script>
    <script src="../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=30646c52"></script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/backward_search_approximated/utils/nodes';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">IPE Documentation</p>
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../guides/index.html">
    User Guides
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../api/index.html">
    API Reference
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/andreac01/IPE-LatentCircuitIdentification.git" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fab fa-github-square fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../guides/index.html">
    User Guides
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../api/index.html">
    API Reference
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/andreac01/IPE-LatentCircuitIdentification.git" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fab fa-github-square fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">backward_search_approximated.utils.nodes</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for backward_search_approximated.utils.nodes</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">abc</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformer_lens</span><span class="w"> </span><span class="kn">import</span> <span class="n">HookedTransformer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformer_lens.HookedTransformerConfig</span><span class="w"> </span><span class="kn">import</span> <span class="n">HookedTransformerConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">backward_search_approximated.utils.attention</span><span class="w"> </span><span class="kn">import</span> <span class="n">custom_attention_forward</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">total_ordering</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Optional</span>


<div class="viewcode-block" id="ApproxNode">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.ApproxNode">[docs]</a>
<span class="nd">@total_ordering</span>
<span class="k">class</span><span class="w"> </span><span class="nc">ApproxNode</span><span class="p">(</span><span class="n">abc</span><span class="o">.</span><span class="n">ABC</span><span class="p">):</span>
<span class="w">	</span><span class="sd">&quot;&quot;&quot;Abstract base class for representing computational nodes in a transformer model.</span>
<span class="sd">	</span>
<span class="sd">	Implements functionality aimed at providing an unified interface for calculating:</span>
<span class="sd">	- The effect of an input message on the output of the node (forward method)</span>
<span class="sd">	- The list of predecessor nodes in the computational graph (get_expansion_candidates method)</span>
<span class="sd">	- The gradient of the final output of a the path with respect to the input of this node (calculate_gradient method)</span>

<span class="sd">	Attributes:</span>
<span class="sd">		model (HookedTransformer): </span>
<span class="sd">			The transformer model instance. It is assumed to be a HookedTransformer from transformer_lens library. Any other implementation which provide the same interface should work as well.</span>
<span class="sd">		layer (int): </span>
<span class="sd">			Layer index in the transformer. Embedding layer is assumed to be layer 0.</span>
<span class="sd">		position (int, default=None): </span>
<span class="sd">			Token position if position-specific, else None. None is equivalent to all positions.</span>
<span class="sd">		parent (ApproxNode, default=None): </span>
<span class="sd">			Parent node in the next node in the path. The parent is a successor in the computational graph.</span>
<span class="sd">		children (set, default=set()): </span>
<span class="sd">			Set of child nodes. A child is a predecessor in the computational graph.</span>
<span class="sd">		msg_cache (dict): </span>
<span class="sd">			Clean activation cache. Can be obtained by running the model with hooks using the clean prompt and converting the result to a dictionary. It must be a dictionary because it might be modified by adding new cached entries, corresponding to the outputs of subcomponents (e.g. single attention heads). </span>
<span class="sd">		cf_cache (dict, default={}): </span>
<span class="sd">			Counterfactual activation cache. Can be obtained by running the model with hooks using the corrupted prompt and converting the result to a dictionary. It must be a dictionary because it might be modified by adding new cached entries, corresponding to the outputs of subcomponents (e.g. single attention heads).</span>
<span class="sd">		gradient (Tensor, default=None): </span>
<span class="sd">			Node cached gradient. Usually is used to represent the gradient of the final output with respect to the input of this node, passing trough the path from final node to the current one.</span>
<span class="sd">		input_name (str): </span>
<span class="sd">			Input activation name. This is the name associated to the cache entry corresponding to the input of this node.</span>
<span class="sd">		output_name (str): </span>
<span class="sd">			Output activation name. This is the name associated to the cache entry corresponding to the output of this node.</span>
<span class="sd">		patch_type (str, default=&#39;zero&#39;): </span>
<span class="sd">			Type of intervention (&#39;zero&#39; or &#39;counterfactual&#39;). Zero patching corresponds to removing the message from the first node in the path to the input of the next node, while counterfactual patching corresponds to replacing the message with the counterfactual activation. In both cases the effect of the path is then calculated by propagating the message through the whole path.</span>

<span class="sd">	Notes:</span>
<span class="sd">		This is an abstract base class that should not be instantiated directly. Concrete implementations should inherit from this class and implement the required abstract methods.</span>
<span class="sd">	&quot;&quot;&quot;</span>
<div class="viewcode-block" id="ApproxNode.__init__">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.ApproxNode.__init__">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">HookedTransformer</span><span class="p">,</span> <span class="n">layer</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">msg_cache</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">input_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">output_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">position</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">cf_cache</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{},</span> <span class="n">parent</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">children</span><span class="p">:</span> <span class="nb">set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(),</span> <span class="n">gradient</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">patch_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;zero&#39;</span><span class="p">):</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;	 Initializes an ApproxNode instance.</span>

<span class="sd">		Args:</span>
<span class="sd">			model (HookedTransformer): </span>
<span class="sd">				The transformer model instance. It is assumed to be a HookedTransformer from transformer_lens library. Any other implementation which provide the same interface should work as well.</span>
<span class="sd">			layer (int): </span>
<span class="sd">				Layer index in the transformer. Embedding layer is assumed to be layer 0.</span>
<span class="sd">			position (int, default=None): </span>
<span class="sd">				Token position if position-specific, else None. None is equivalent to all positions.</span>
<span class="sd">			parent (ApproxNode, default=None): </span>
<span class="sd">				Parent node in the next node in the path. The parent is a successor in the computational graph.</span>
<span class="sd">			children (set, default=set()): </span>
<span class="sd">				Set of child nodes. A child is a predecessor in the computational graph.</span>
<span class="sd">			msg_cache (dict): </span>
<span class="sd">				Clean activation cache. Can be obtained by running the model with hooks using the clean prompt and converting the result to a dictionary. It must be a dictionary because it might be modified by adding new cached entries, corresponding to the outputs of subcomponents (e.g. single attention heads). </span>
<span class="sd">			cf_cache (dict, default={}): </span>
<span class="sd">				Counterfactual activation cache. Can be obtained by running the model with hooks using the corrupted prompt and converting the result to a dictionary. It must be a dictionary because it might be modified by adding new cached entries, corresponding to the outputs of subcomponents (e.g. single attention heads).</span>
<span class="sd">			gradient (Tensor, default=None): </span>
<span class="sd">				Node cached gradient. Usually is used to represent the gradient of the final output with respect to the input of this node, passing trough the path from final node to the current one.</span>
<span class="sd">			input_name (str): </span>
<span class="sd">				Input activation name. This is the name associated to the cache entry corresponding to the input of this node.</span>
<span class="sd">			output_name (str): </span>
<span class="sd">				Output activation name. This is the name associated to the cache entry corresponding to the output of this node.</span>
<span class="sd">			patch_type (str, default=&#39;zero&#39;): </span>
<span class="sd">				Type of intervention (&#39;zero&#39; or &#39;counterfactual&#39;). Zero patching corresponds to removing the message from the first node in the path to the input of the next node, while counterfactual patching corresponds to replacing the message with the counterfactual activation. In both cases the effect of the path is then calculated by propagating the message through the whole path.</span>
<span class="sd">		Returns:</span>
<span class="sd">			ApproxNode:</span>
<span class="sd">				An instance of ApproxNode.</span>
<span class="sd">	&quot;&quot;&quot;</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">layer</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="o">=</span> <span class="n">position</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">parent</span><span class="o">=</span> <span class="n">parent</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">children</span> <span class="o">=</span> <span class="n">children</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span> <span class="o">=</span> <span class="n">msg_cache</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">cf_cache</span> <span class="o">=</span> <span class="n">cf_cache</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">gradient</span> <span class="o">=</span> <span class="n">gradient</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">input_name</span> <span class="o">=</span> <span class="n">input_name</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">output_name</span> <span class="o">=</span> <span class="n">output_name</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">patch_type</span> <span class="o">=</span> <span class="n">patch_type</span>
		<span class="k">if</span> <span class="n">patch_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;zero&#39;</span><span class="p">,</span> <span class="s1">&#39;counterfactual&#39;</span><span class="p">]:</span>
			<span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown patch type: </span><span class="si">{</span><span class="n">patch_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApproxNode.add_child">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.ApproxNode.add_child">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="nf">add_child</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">child</span><span class="p">:</span> <span class="s1">&#39;ApproxNode&#39;</span><span class="p">):</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;Adds a node as a child of self and sets self as its parent. A child can be interpreted as a predecessor in the computational graph.</span>
<span class="sd">		</span>
<span class="sd">		Args:</span>
<span class="sd">			child (ApproxNode)</span>
<span class="sd">				The ApproxNode to be added as a child of self.</span>
<span class="sd">		</span>
<span class="sd">		Returns:</span>
<span class="sd">			None</span>

<span class="sd">		&quot;&quot;&quot;</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
		<span class="n">child</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApproxNode.add_parent">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.ApproxNode.add_parent">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="nf">add_parent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parent</span><span class="p">:</span> <span class="s1">&#39;ApproxNode&#39;</span><span class="p">):</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;Adds a node as a parent of self and update the list of children of the parent node. A parent can be interpreted as a successor in the computational graph.</span>
<span class="sd">		</span>
<span class="sd">		Args:</span>
<span class="sd">			parent (ApproxNode):</span>
<span class="sd">				The ApproxNode to be added as a parent of self.</span>
<span class="sd">		</span>
<span class="sd">		Returns:</span>
<span class="sd">			None</span>
<span class="sd">		</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">parent</span><span class="p">)</span>
		<span class="n">parent</span><span class="o">.</span><span class="n">children</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApproxNode.forward">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.ApproxNode.forward">[docs]</a>
	<span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
	<span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">message</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">		Calculate the effect of the message on the output of the node. </span>
<span class="sd">		</span>
<span class="sd">		The effect is calculated indirectly as the difference between the normal output of the component and the </span>
<span class="sd">		one obtained when the message is removed from the input of the node.</span>
<span class="sd">		On the other hand, if message is None the behavior depends on the patch_type:</span>
<span class="sd">		- &#39;zero&#39;: returns the normal output of the component</span>
<span class="sd">		- &#39;counterfactual&#39;: returns the difference between the normal output and the counterfactual output of the component</span>

<span class="sd">		Args:</span>

<span class="sd">			message (Tensor of shape (batch_size, seq_len, d_model), default=None):</span>
<span class="sd">				The message whose effect on the node need to be evaluated. If None, returns the normal </span>
<span class="sd">				output or the difference between normal and counterfactual output depending on patch_type.</span>
<span class="sd">			</span>
<span class="sd">		Returns:</span>
<span class="sd">			Tensor:</span>
<span class="sd">				A tensor representing the effect of the message on the output of the node.</span>
<span class="sd">				In simpler terms, it represents the message caused by passing the input message through this node.</span>

<span class="sd">		Notes:</span>
<span class="sd">		- If a position is specified the output will be zero for all other positions.</span>
<span class="sd">		- The method assumes that the msg_cache and cf_cache contain the necessary activations.</span>
<span class="sd">		- When message is None, the method will cache the output in msg_cache or cf_cache if not already present.</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">pass</span></div>


<div class="viewcode-block" id="ApproxNode.get_expansion_candidates">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.ApproxNode.get_expansion_candidates">[docs]</a>
	<span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
	<span class="k">def</span><span class="w"> </span><span class="nf">get_expansion_candidates</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_cfg</span><span class="p">:</span> <span class="n">HookedTransformerConfig</span><span class="p">,</span> <span class="n">include_head</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">separate_kv</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="s1">&#39;ApproxNode&#39;</span><span class="p">]:</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">		Returns the list of predecessors nodes in the computational graph whose outputs influence the</span>
<span class="sd">		output of this node. </span>

<span class="sd">		Args:</span>

<span class="sd">			model_cfg (HookedTransformerConfig):</span>
<span class="sd">				The configuration of the transformer model. It is used to determine the number of heads and other model parameters.</span>

<span class="sd">			include_head (bool, default=False):</span>
<span class="sd">				Whether to consider specific head nodes for ATTN.</span>

<span class="sd">			separate_kv (bool, default=False):</span>
<span class="sd">				Whether to consider key and value positions separately for ATTN nodes </span>
<span class="sd">		</span>
<span class="sd">		Returns:</span>
<span class="sd">			list of ApproxNode:</span>
<span class="sd">				The list of all predecessor nodes infuencing the input of this node.</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">pass</span></div>


<div class="viewcode-block" id="ApproxNode.calculate_gradient">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.ApproxNode.calculate_gradient">[docs]</a>
	<span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
	<span class="k">def</span><span class="w"> </span><span class="nf">calculate_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_precomputed</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">		Calculates the gradient of the node&#39;s input with respect to the final output.</span>
<span class="sd">		By default the gradient is calculated propagating backwards from the parent node if present,</span>
<span class="sd">		or assuming a gradient of ones if self has no parent. When &#39;grad_outputs&#39; is specified, it is used instead of the parent&#39;s gradient.</span>

<span class="sd">		Args:</span>
<span class="sd">			grad_outputs : Tensor, optional (default=None)</span>
<span class="sd">				Gradient to propagate backwards. If None, uses the gradient from the parent node or ones.</span>
<span class="sd">			</span>
<span class="sd">			save : bool, optional (default=True)</span>
<span class="sd">				Whether to save the computed gradient in self.gradient. The gradient can be reused</span>
<span class="sd">				later by setting use_precomputed to True.</span>
<span class="sd">			</span>
<span class="sd">			use_precomputed : bool, optional (default=False)</span>
<span class="sd">				Whether to use the precomputed gradient if available. The precoputed gradient is stored whenever</span>
<span class="sd">				save is True.</span>
<span class="sd">		</span>
<span class="sd">		Returns:</span>
<span class="sd">			gradient : Tensor</span>
<span class="sd">				A tensor representing the gradient of the output with respect to the input</span>
<span class="sd">				of this node, passing trough the path from final node to the current one.</span>
<span class="sd">		&quot;&quot;&quot;</span></div>



<div class="viewcode-block" id="ApproxNode.__repr__">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.ApproxNode.__repr__">[docs]</a>
	<span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
	<span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">		Returns a string representation of the node. </span>

<span class="sd">		Returns:</span>
<span class="sd">			str:</span>
<span class="sd">				A string representation of the node.</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">pass</span></div>


	<span class="k">def</span><span class="w"> </span><span class="nf">_get_sort_key</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;Helper method to return a tuple for sorting.</span>
<span class="sd">		Sorting order:</span>
<span class="sd">		1. Layer (ascending)</span>
<span class="sd">		2. Position (ascending, None last)</span>
<span class="sd">		3. Node type (EMBED &lt; ATTN &lt; MLP &lt; FINAL)</span>
<span class="sd">		4. Key/Value position (ascending, None last)</span>
<span class="sd">		5. Head (ascending, None last)</span>

<span class="sd">		Returns:</span>
<span class="sd">			tuple: </span>
<span class="sd">				A tuple representing the sort key.</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="c1"># Define an order for node types</span>
		<span class="n">type_order</span> <span class="o">=</span> <span class="p">{</span><span class="n">EMBED_ApproxNode</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">ATTN_ApproxNode</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MLP_ApproxNode</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="n">FINAL_ApproxNode</span><span class="p">:</span> <span class="mi">3</span><span class="p">}</span>
		<span class="n">node_type</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
		<span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span>
		<span class="n">pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span>
		<span class="n">keyvalue_position</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;keyvalue_position&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
		<span class="n">head</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;head&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
		<span class="n">head</span> <span class="o">=</span> <span class="n">head</span> <span class="k">if</span> <span class="n">head</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span>

		<span class="k">return</span> <span class="p">(</span>
			<span class="n">layer</span><span class="p">,</span>
			<span class="n">pos</span><span class="p">,</span>
			<span class="n">type_order</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">node_type</span><span class="p">,</span> <span class="mi">99</span><span class="p">),</span>
			<span class="n">keyvalue_position</span><span class="p">,</span>
			<span class="n">head</span>
		<span class="p">)</span>

<div class="viewcode-block" id="ApproxNode.__lt__">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.ApproxNode.__lt__">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="fm">__lt__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;Defines a total ordering for ApproxNode instances based on layer, position, type, key/value position, and head.</span>
<span class="sd">		Args:</span>
<span class="sd">			other (ApproxNode): The other ApproxNode instance to compare with.</span>
<span class="sd">		Returns:</span>
<span class="sd">			bool: True if self is less than other, False otherwise.</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">ApproxNode</span><span class="p">):</span>
			<span class="k">return</span> <span class="bp">NotImplemented</span>
		<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_sort_key</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">other</span><span class="o">.</span><span class="n">_get_sort_key</span><span class="p">()</span></div>


<div class="viewcode-block" id="ApproxNode.__eq__">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.ApproxNode.__eq__">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="fm">__eq__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;Checks equality between two ApproxNode instances based on layer, position, type, key/value position, and head.</span>
<span class="sd">		Args:</span>
<span class="sd">			other (ApproxNode): The other ApproxNode instance to compare with.</span>
<span class="sd">		Returns:</span>
<span class="sd">			bool: True if self is equal to other, False otherwise.</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">ApproxNode</span><span class="p">):</span>
			<span class="k">return</span> <span class="bp">NotImplemented</span>
		<span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span> <span class="o">!=</span> <span class="n">other</span><span class="o">.</span><span class="n">layer</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="o">!=</span> <span class="n">other</span><span class="o">.</span><span class="n">position</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">type</span><span class="p">(</span><span class="n">other</span><span class="p">)):</span>
			<span class="k">return</span> <span class="kc">False</span>
		<span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ATTN_ApproxNode</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">ATTN_ApproxNode</span><span class="p">):</span>
			<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">head</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">position</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">keyvalue_position</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_key</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">patch_key</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_query</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">patch_query</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_value</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">patch_value</span>
		<span class="k">return</span> <span class="kc">True</span></div>

		

<div class="viewcode-block" id="ApproxNode.__hash__">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.ApproxNode.__hash__">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="fm">__hash__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;Generates a hash based on layer, position and type.</span>
<span class="sd">		Returns:</span>
<span class="sd">			int: The hash value of the ApproxNode instance.</span>
<span class="sd">		Notes:</span>
<span class="sd">			This methos is overridden in child classes to include additional attributes.</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">return</span> <span class="nb">hash</span><span class="p">((</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">))</span></div>
</div>


<div class="viewcode-block" id="MLP_ApproxNode">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.MLP_ApproxNode">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">MLP_ApproxNode</span><span class="p">(</span><span class="n">ApproxNode</span><span class="p">):</span>
<span class="w">	</span><span class="sd">&quot;&quot;&quot;Represents an Multi Layer Perceptron (also referred as Feed-Forward Networl) node in the transformer. </span>
<span class="sd">	This node operates on the residual stream within a specific layer and position.</span>
<span class="sd">	Note that an MLP output in a specific position is independent from the outputs in other positions, allowing for easier caching and patching.</span>
<span class="sd">	</span>
<span class="sd">	Attributes:</span>
<span class="sd">		model (HookedTransformer): </span>
<span class="sd">			The transformer model instance. It is assumed to be a HookedTransformer from transformer_lens library. Any other implementation which provide the same interface should work as well.</span>
<span class="sd">		layer (int): </span>
<span class="sd">			Layer index in the transformer. Embedding layer is assumed to be layer 0.</span>
<span class="sd">		position (int, default=None): </span>
<span class="sd">			Token position if position-specific, else None. None is equivalent to all positions.</span>
<span class="sd">		parent (ApproxNode, default=None): </span>
<span class="sd">			Parent node in the next node in the path. The parent is a successor in the computational graph.</span>
<span class="sd">		children (set, default=set()): </span>
<span class="sd">			Set of child nodes. A child is a predecessor in the computational graph.</span>
<span class="sd">		msg_cache (dict): </span>
<span class="sd">			Clean activation cache. Can be obtained by running the model with hooks using the clean prompt and converting the result to a dictionary. It must be a dictionary because it might be modified by adding new cached entries, corresponding to the outputs of subcomponents (e.g. single attention heads). </span>
<span class="sd">		cf_cache (dict, default={}): </span>
<span class="sd">			Counterfactual activation cache. Can be obtained by running the model with hooks using the corrupted prompt and converting the result to a dictionary. It must be a dictionary because it might be modified by adding new cached entries, corresponding to the outputs of subcomponents (e.g. single attention heads).</span>
<span class="sd">		gradient (Tensor, default=None): </span>
<span class="sd">			Node cached gradient. Usually is used to represent the gradient of the final output with respect to the input of this node, passing trough the path from final node to the current one.</span>
<span class="sd">		input_name (str): </span>
<span class="sd">			Input activation name. This is the name associated to the cache entry corresponding to the input of this node.</span>
<span class="sd">		output_name (str): </span>
<span class="sd">			Output activation name. This is the name associated to the cache entry corresponding to the output of this node.</span>
<span class="sd">		patch_type (str, default=&#39;zero&#39;): </span>
<span class="sd">			Type of intervention (&#39;zero&#39; or &#39;counterfactual&#39;). Zero patching corresponds to removing the message from the first node in the path to the input of the next node, while counterfactual patching corresponds to replacing the message with the counterfactual activation. In both cases the effect of the path is then calculated by propagating the message through the whole path.</span>
<span class="sd">	&quot;&quot;&quot;</span>
<div class="viewcode-block" id="MLP_ApproxNode.__init__">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.MLP_ApproxNode.__init__">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">HookedTransformer</span><span class="p">,</span> <span class="n">layer</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">position</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">parent</span><span class="p">:</span> <span class="n">ApproxNode</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">children</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(),</span> <span class="n">msg_cache</span> <span class="o">=</span> <span class="p">{},</span> <span class="n">cf_cache</span> <span class="o">=</span> <span class="p">{},</span> <span class="n">gradient</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">patch_type</span> <span class="o">=</span> <span class="s1">&#39;zero&#39;</span><span class="p">):</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;Initializes an MLP_ApproxNode instance.</span>
<span class="sd">		Args:</span>
<span class="sd">			model (HookedTransformer): </span>
<span class="sd">				The transformer model instance. It is assumed to be a HookedTransformer from transformer_lens library. Any other implementation which provide the same interface should work as well.</span>
<span class="sd">			layer (int): </span>
<span class="sd">				Layer index in the transformer. Embedding layer is assumed to be layer 0.</span>
<span class="sd">			position (int, default=None): </span>
<span class="sd">				Token position if position-specific, else None. None is equivalent to all positions.</span>
<span class="sd">			parent (ApproxNode, default=None): </span>
<span class="sd">				Parent node in the next node in the path. The parent is a successor in the computational graph.</span>
<span class="sd">			children (set, default=set()): </span>
<span class="sd">				Set of child nodes. A child is a predecessor in the computational graph.</span>
<span class="sd">			msg_cache (dict): </span>
<span class="sd">				Clean activation cache. Can be obtained by running the model with hooks using the clean prompt and converting the result to a dictionary. It must be a dictionary because it might be modified by adding new cached entries, corresponding to the outputs of subcomponents (e.g. single attention heads). </span>
<span class="sd">			cf_cache (dict, default={}): </span>
<span class="sd">				Counterfactual activation cache. Can be obtained by running the model with hooks using the corrupted prompt and converting the result to a dictionary. It must be a dictionary because it might be modified by adding new cached entries, corresponding to the outputs of subcomponents (e.g. single attention heads).</span>
<span class="sd">			gradient (Tensor, default=None): </span>
<span class="sd">				Node cached gradient. Usually is used to represent the gradient of the final output with respect to the input of this node, passing trough the path from final node to the current one.</span>
<span class="sd">			patch_type (str, default=&#39;zero&#39;): </span>
<span class="sd">				Type of intervention (&#39;zero&#39; or &#39;counterfactual&#39;). Zero patching corresponds to removing the message from the first node in the path to the input of the next node, while counterfactual patching corresponds to replacing the message with the counterfactual activation. In both cases the effect of the path is then calculated by propagating the message through the whole path.</span>
<span class="sd">		Returns:</span>
<span class="sd">			self (MLP_ApproxNode):</span>
<span class="sd">				The initialized MLP_ApproxNode instance.</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span> <span class="n">parent</span><span class="o">=</span><span class="n">parent</span><span class="p">,</span> <span class="n">children</span><span class="o">=</span><span class="n">children</span><span class="p">,</span> <span class="n">msg_cache</span><span class="o">=</span><span class="n">msg_cache</span><span class="p">,</span> <span class="n">cf_cache</span><span class="o">=</span><span class="n">cf_cache</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient</span><span class="p">,</span> <span class="n">input_name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;blocks.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">.hook_resid_mid&quot;</span><span class="p">,</span> <span class="n">output_name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;blocks.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">.hook_mlp_out&quot;</span><span class="p">,</span> <span class="n">patch_type</span><span class="o">=</span><span class="n">patch_type</span><span class="p">)</span></div>

	

<div class="viewcode-block" id="MLP_ApproxNode.forward">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.MLP_ApproxNode.forward">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">message</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">		Calculate the effect of the message on the output of the node. </span>
<span class="sd">		</span>
<span class="sd">		The effect is calculated indirectly as the difference between the normal output of the MLP and the </span>
<span class="sd">		one obtained when the message is removed from the input of the node.</span>
<span class="sd">		On the other hand, if message is None the behavior depends on the patch_type:</span>
<span class="sd">		- &#39;zero&#39;: returns the normal output of the MLP</span>
<span class="sd">		- &#39;counterfactual&#39;: returns the difference between the normal output and the counterfactual output</span>

<span class="sd">		Args:</span>

<span class="sd">			message (Tensor of shape (batch_size, seq_len, d_model), default=None):</span>
<span class="sd">				The message whose effect on the node need to be evaluated. If None, returns the normal </span>
<span class="sd">				output or the difference between normal and counterfactual output depending on patch_type.</span>
<span class="sd">			</span>
<span class="sd">		Returns:</span>
<span class="sd">			Tensor:</span>
<span class="sd">				A tensor representing the effect of the message on the output of the node.</span>

<span class="sd">		Notes:</span>
<span class="sd">		- If a position is specified the output will be zero for all other positions.</span>
<span class="sd">		- The method assumes that the msg_cache and cf_cache contain the necessary activations.</span>
<span class="sd">		- When message is None, the method will cache the output in msg_cache or cf_cache if not already present.</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">if</span> <span class="n">message</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
			<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_type</span> <span class="o">==</span> <span class="s1">&#39;zero&#39;</span><span class="p">:</span>
				<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
					<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">output_name</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
				<span class="k">else</span><span class="p">:</span>
					<span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">]</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
					<span class="n">out</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">output_name</span><span class="p">][:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
					<span class="k">return</span> <span class="n">out</span>
			<span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_type</span> <span class="o">==</span> <span class="s1">&#39;counterfactual&#39;</span><span class="p">:</span>
				<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
					<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">output_name</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">cf_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">output_name</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
				<span class="k">else</span><span class="p">:</span>
					<span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">]</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
					<span class="n">out</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span>  <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">output_name</span><span class="p">][:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">cf_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">output_name</span><span class="p">][:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
					<span class="k">return</span> <span class="n">out</span>
		<span class="k">else</span><span class="p">:</span>
			<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
				<span class="n">residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="o">-</span> <span class="n">message</span>
				<span class="n">residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">ln2</span><span class="p">(</span><span class="n">residual</span><span class="p">)</span>
				<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">output_name</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">mlp</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">residual</span><span class="p">)</span>
			<span class="k">else</span><span class="p">:</span>
				<span class="n">residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">][:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="o">-</span> <span class="n">message</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span>
				<span class="n">residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">ln2</span><span class="p">(</span><span class="n">residual</span><span class="p">)</span>
				<span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">]</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
				<span class="n">out</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">output_name</span><span class="p">][:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">mlp</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">residual</span><span class="p">)</span>
				<span class="k">return</span> <span class="n">out</span></div>



<div class="viewcode-block" id="MLP_ApproxNode.get_expansion_candidates">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.MLP_ApproxNode.get_expansion_candidates">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="nf">get_expansion_candidates</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_cfg</span><span class="p">:</span> <span class="n">HookedTransformerConfig</span><span class="p">,</span> <span class="n">include_head</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">separate_kv</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">ApproxNode</span><span class="p">]:</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">		Returns the list of predecessors nodes in the computational graph whose outputs influence the</span>
<span class="sd">		output of this node. </span>
<span class="sd">		Previous nodes of an MLP are:</span>
<span class="sd">			- MLP, EMBED and ATTN nodes in self.position from previous layers.</span>
<span class="sd">			- ATTN nodes in self.position from current layers.</span>
<span class="sd">		ATTN nodes are always patched both in query and key-value positions separately.</span>
<span class="sd">		Args:</span>

<span class="sd">			model_cfg (HookedTransformerConfig):</span>
<span class="sd">				The configuration of the transformer model. It is used to determine the number of heads and other model parameters.</span>

<span class="sd">			include_head (bool, default=False):</span>
<span class="sd">				Whether to consider specific head nodes for ATTN.</span>

<span class="sd">			separate_kv (bool, default=False):</span>
<span class="sd">				Whether to consider key and value positions separately for ATTN nodes </span>
<span class="sd">		</span>
<span class="sd">		Returns:</span>
<span class="sd">			list of ApproxNode:</span>
<span class="sd">				The list of all predecessor nodes infuencing the input of this node.</span>
<span class="sd">		</span>
<span class="sd">		Notes:</span>
<span class="sd">			- If self.position is None, only non-position-specific previous nodes are considered.</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="n">prev_nodes</span> <span class="o">=</span> <span class="p">[]</span>
		<span class="n">common_args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;msg_cache&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">,</span> <span class="s2">&quot;cf_cache&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">cf_cache</span><span class="p">,</span> <span class="s2">&quot;parent&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;patch_type&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_type</span><span class="p">}</span>
		<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
			<span class="n">positions_to_iterate</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
		<span class="k">else</span><span class="p">:</span>
			<span class="n">positions_to_iterate</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span>

		<span class="c1"># MLP and ATTN nodes from previous layers</span>
		<span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">):</span>
			<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">MLP_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>
			<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">positions_to_iterate</span><span class="p">:</span>
				<span class="k">if</span> <span class="n">include_head</span><span class="p">:</span>
					<span class="k">if</span> <span class="n">separate_kv</span><span class="p">:</span>
						<span class="n">prev_nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model_cfg</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)])</span>
						<span class="n">prev_nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model_cfg</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)])</span>
					<span class="k">else</span><span class="p">:</span>
						<span class="n">prev_nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model_cfg</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)])</span>
				<span class="k">else</span><span class="p">:</span>
					<span class="k">if</span> <span class="n">separate_kv</span><span class="p">:</span>
						<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>
						<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>
					<span class="k">else</span><span class="p">:</span>
						<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>
			<span class="k">if</span> <span class="n">include_head</span><span class="p">:</span>
				<span class="n">prev_nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="n">patch_key</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model_cfg</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)])</span>
			<span class="k">else</span><span class="p">:</span>
				<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="n">patch_key</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>

		<span class="c1"># ATTN nodes from current layer</span>
		<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">positions_to_iterate</span><span class="p">:</span>
			<span class="k">if</span> <span class="n">include_head</span><span class="p">:</span>
				<span class="k">if</span> <span class="n">separate_kv</span><span class="p">:</span>
					<span class="n">prev_nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model_cfg</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)])</span>
					<span class="n">prev_nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model_cfg</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)])</span>
				<span class="k">else</span><span class="p">:</span>
					<span class="n">prev_nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model_cfg</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)])</span>
			<span class="k">else</span><span class="p">:</span>
				<span class="k">if</span> <span class="n">separate_kv</span><span class="p">:</span>
					<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>
					<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>
				<span class="k">else</span><span class="p">:</span>
					<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>
		<span class="k">if</span> <span class="n">include_head</span><span class="p">:</span>
			<span class="n">prev_nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="n">patch_key</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model_cfg</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)])</span>
		<span class="k">else</span><span class="p">:</span>
			<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="n">patch_key</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>

		<span class="c1"># EMBED node</span>
		<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">EMBED_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>
		<span class="c1"># Remove duplicates</span>
		<span class="n">prev_nodes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">prev_nodes</span><span class="p">))</span>
		<span class="k">return</span> <span class="n">prev_nodes</span></div>


<div class="viewcode-block" id="MLP_ApproxNode.__repr__">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.MLP_ApproxNode.__repr__">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;Returns a string representation of the MLP node.</span>
<span class="sd">		Returns:</span>
<span class="sd">			str:</span>
<span class="sd">				A string representation of the MLP node.</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;MLP_ApproxNode(layer=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="si">}</span><span class="s2">, position=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="si">}</span><span class="s2">)&quot;</span></div>


<div class="viewcode-block" id="MLP_ApproxNode.__hash__">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.MLP_ApproxNode.__hash__">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="fm">__hash__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;Generates a hash based on layer, position and type.</span>
<span class="sd">		Returns:</span>
<span class="sd">			int: The hash value of the MLP_ApproxNode instance.</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">return</span> <span class="nb">hash</span><span class="p">((</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">))</span></div>

	
<div class="viewcode-block" id="MLP_ApproxNode.calculate_gradient">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.MLP_ApproxNode.calculate_gradient">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="nf">calculate_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_precomputed</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">		Calculates the gradient of the node&#39;s input with respect to the final output.</span>
<span class="sd">		By default the gradient is calculated propagating backwards from the parent node if present,</span>
<span class="sd">		or assuming a gradient of ones if self has no parent. When &#39;grad_outputs&#39; is specified, it is used instead of the parent&#39;s gradient.</span>

<span class="sd">		Args:</span>
<span class="sd">			grad_outputs : Tensor, optional (default=None)</span>
<span class="sd">				Gradient to propagate backwards. If None, uses the gradient from the parent node or ones.</span>
<span class="sd">			</span>
<span class="sd">			save : bool, optional (default=True)</span>
<span class="sd">				Whether to save the computed gradient in self.gradient. The gradient can be reused</span>
<span class="sd">				later by setting use_precomputed to True.</span>
<span class="sd">			</span>
<span class="sd">			use_precomputed : bool, optional (default=False)</span>
<span class="sd">				Whether to use the precomputed gradient if available. The precoputed gradient is stored whenever</span>
<span class="sd">				save is True.</span>
<span class="sd">		</span>
<span class="sd">		Returns:</span>
<span class="sd">			gradient : Tensor</span>
<span class="sd">				A tensor representing the gradient of the output with respect to the input</span>
<span class="sd">				of this node, passing trough the path from final node to the current one.</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">use_precomputed</span><span class="p">:</span>
			<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
				<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
			<span class="n">gradient</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
			<span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">gradient</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
			<span class="n">out</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">gradient</span>
			<span class="k">return</span> <span class="n">out</span>

		<span class="n">input_residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
		<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
			<span class="n">input_residual</span> <span class="o">=</span> <span class="n">input_residual</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span>
		<span class="n">input_residual</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

		<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
			<span class="n">norm_res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">ln2</span><span class="p">(</span><span class="n">input_residual</span><span class="p">)</span>
			<span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">mlp</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">norm_res</span><span class="p">)</span>
		
		<span class="k">if</span> <span class="n">grad_outputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
			<span class="n">grad_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">calculate_gradient</span><span class="p">(</span><span class="n">save</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_precomputed</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">input_residual</span><span class="p">)</span>
	
		<span class="k">if</span> <span class="n">input_residual</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">grad_outputs</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
			<span class="n">grad_outputs</span> <span class="o">=</span> <span class="n">grad_outputs</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span>
		<span class="n">gradient</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span>
			<span class="n">output</span><span class="p">,</span>
			<span class="n">input_residual</span><span class="p">,</span>
			<span class="n">grad_outputs</span><span class="o">=</span><span class="n">grad_outputs</span><span class="p">,</span>
		<span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
		<span class="k">if</span> <span class="n">save</span><span class="p">:</span>
			<span class="bp">self</span><span class="o">.</span><span class="n">gradient</span> <span class="o">=</span> <span class="n">gradient</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
		<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
			<span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">gradient</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
			<span class="n">out</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">gradient</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
			<span class="k">return</span> <span class="n">out</span>
		<span class="k">return</span> <span class="n">gradient</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span></div>
</div>

	

<div class="viewcode-block" id="ATTN_ApproxNode">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.ATTN_ApproxNode">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">ATTN_ApproxNode</span><span class="p">(</span><span class="n">ApproxNode</span><span class="p">):</span>
<span class="w">	</span><span class="sd">&quot;&quot;&quot;Represents an Attention node (potentially a specific head) in the transformer model.</span>

<span class="sd">	Attributes:</span>
<span class="sd">		model (HookedTransformer): </span>
<span class="sd">			The transformer model instance. It is assumed to be a HookedTransformer from transformer_lens library. Any other implementation which provide the same interface should work as well.</span>
<span class="sd">		layer (int): </span>
<span class="sd">			Layer index in the transformer. Embedding layer is assumed to be layer 0.</span>
<span class="sd">		head (int, default=None):</span>
<span class="sd">			Attention head index if head-specific, else None. None is equivalent to all heads. When an head is specified the contribution of the is considered, particularly the bias term which is not head-specific is not included. Therefore the output of an ATTN node is equal to the output of all the heads plus the bias term. If head is None the whole attention output is considered.</span>
<span class="sd">		position (int, default=None): </span>
<span class="sd">			Token position if position-specific, else None. None is equivalent to all positions.</span>
<span class="sd">		keyvalue_position (int, default=None):</span>
<span class="sd">			Key/Value token position if position-specific, else None. None is equivalent to all positions.</span>
<span class="sd">			If keyvalue_position is specified, the node represents the contribution of the attention head when the value residual strams of all other positions are zeroed out. This is equivalent to attending only to a single position, but scaling the output by the attention score of that position.</span>
<span class="sd">		patch_key (bool, default=True):</span>
<span class="sd">			Whether to patch the key projection of the attention head. If False, the key projection is not patched and the message is only removed from the query and/or value projections.</span>
<span class="sd">		patch_value (bool, default=True):</span>
<span class="sd">			Whether to patch the value projection of the attention head. If False, the value projection is not patched and the message is only removed from the query and/or key projections.</span>
<span class="sd">		patch_query (bool, default=True):</span>
<span class="sd">			Whether to patch the query projection of the attention head. If False, the query projection is not patched and the message is only removed from the key and/or value projections.</span>
<span class="sd">		parent (ApproxNode, default=None): </span>
<span class="sd">			Parent node in the next node in the path. The parent is a successor in the computational graph.</span>
<span class="sd">		children (set, default=set()): </span>
<span class="sd">			Set of child nodes. A child is a predecessor in the computational graph.</span>
<span class="sd">		msg_cache (dict): </span>
<span class="sd">			Clean activation cache. Can be obtained by running the model with hooks using the clean prompt and converting the result to a dictionary. It must be a dictionary because it might be modified by adding new cached entries, corresponding to the outputs of subcomponents (e.g. single attention heads). </span>
<span class="sd">		cf_cache (dict, default={}): </span>
<span class="sd">			Counterfactual activation cache. Can be obtained by running the model with hooks using the corrupted prompt and converting the result to a dictionary. It must be a dictionary because it might be modified by adding new cached entries, corresponding to the outputs of subcomponents (e.g. single attention heads).</span>
<span class="sd">		gradient (Tensor, default=None): </span>
<span class="sd">			Node cached gradient. Usually is used to represent the gradient of the final output with respect to the input of this node, passing trough the path from final node to the current one.</span>
<span class="sd">		attn_scores (str):</span>
<span class="sd">			Attention scores activation name. This is the name associated to the cache entry corresponding to the attention scores of this attention block. It is used to only recompute the attention scores of relevant positions when patching, drastically reducing computation.</span>
<span class="sd">		input_name (str): </span>
<span class="sd">			Input activation name. This is the name associated to the cache entry corresponding to the input of this node.</span>
<span class="sd">		output_name (str): </span>
<span class="sd">			Output activation name. This is the name associated to the cache entry corresponding to the output of this node.</span>
<span class="sd">		patch_type (str, default=&#39;zero&#39;): </span>
<span class="sd">			Type of intervention (&#39;zero&#39; or &#39;counterfactual&#39;). Zero patching corresponds to removing the message from the first node in the path to the input of the next node, while counterfactual patching corresponds to replacing the message with the counterfactual activation. In both cases the effect of the path is then calculated by propagating the message through the whole path.</span>
<span class="sd">		plot_patterns (bool, default=False):</span>
<span class="sd">			Whether to plot the attention patterns when calculating the forward pass. This is useful for debugging purposes but also to visualize the changes in the attention patterns when patching specific positions.</span>
<span class="sd">	&quot;&quot;&quot;</span>
<div class="viewcode-block" id="ATTN_ApproxNode.__init__">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.ATTN_ApproxNode.__init__">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">HookedTransformer</span><span class="p">,</span> <span class="n">layer</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">position</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">parent</span><span class="p">:</span> <span class="n">ApproxNode</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">children</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(),</span> <span class="n">msg_cache</span> <span class="o">=</span> <span class="p">{},</span> <span class="n">cf_cache</span> <span class="o">=</span> <span class="p">{},</span> <span class="n">gradient</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">patch_query</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">patch_key</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">patch_value</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">plot_patterns</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">patch_type</span> <span class="o">=</span> <span class="s1">&#39;zero&#39;</span><span class="p">):</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;Initializes an ATTN_ApproxNode instance.</span>

<span class="sd">		Args:</span>
<span class="sd">			model (HookedTransformer): </span>
<span class="sd">				The transformer model instance. It is assumed to be a HookedTransformer from transformer_lens library. Any other implementation which provide the same interface should work as well.</span>
<span class="sd">			layer (int): </span>
<span class="sd">				Layer index in the transformer. Embedding layer is assumed to be layer 0.</span>
<span class="sd">			head (int, default=None):</span>
<span class="sd">				Attention head index if head-specific, else None. None is equivalent to all heads. When an head is specified the contribution of the is considered, particularly the bias term which is not head-specific is not included. Therefore the output of an ATTN node is equal to the output of all the heads plus the bias term. If head is None the whole attention output is considered.</span>
<span class="sd">			position (int, default=None): </span>
<span class="sd">				Token position if position-specific, else None. None is equivalent to all positions.</span>
<span class="sd">			keyvalue_position (int, default=None):</span>
<span class="sd">				Key/Value token position if position-specific, else None. None is equivalent to all positions.</span>
<span class="sd">				If keyvalue_position is specified, the node represents the contribution of the attention head when the value residual strams of all other positions are zeroed out. This is equivalent to attending only to a single position, but scaling the output by the attention score of that position.</span>
<span class="sd">			patch_key (bool, default=True):</span>
<span class="sd">				Whether to patch the key projection of the attention head. If False, the key projection is not patched and the message is only removed from the query and/or value projections.</span>
<span class="sd">			patch_value (bool, default=True):</span>
<span class="sd">				Whether to patch the value projection of the attention head. If False, the value projection is not patched and the message is only removed from the query and/or key projections.</span>
<span class="sd">			patch_query (bool, default=True):</span>
<span class="sd">				Whether to patch the query projection of the attention head. If False, the query projection is not patched and the message is only removed from the key and/or value projections.</span>
<span class="sd">			parent (ApproxNode, default=None): </span>
<span class="sd">				Parent node in the next node in the path. The parent is a successor in the computational graph.</span>
<span class="sd">			children (set, default=set()): </span>
<span class="sd">				Set of child nodes. A child is a predecessor in the computational graph.</span>
<span class="sd">			msg_cache (dict): </span>
<span class="sd">				Clean activation cache. Can be obtained by running the model with hooks using the clean prompt and converting the result to a dictionary. It must be a dictionary because it might be modified by adding new cached entries, corresponding to the outputs of subcomponents (e.g. single attention heads). </span>
<span class="sd">			cf_cache (dict, default={}): </span>
<span class="sd">				Counterfactual activation cache. Can be obtained by running the model with hooks using the corrupted prompt and converting the result to a dictionary. It must be a dictionary because it might be modified by adding new cached entries, corresponding to the outputs of subcomponents (e.g. single attention heads).</span>
<span class="sd">			gradient (Tensor, default=None): </span>
<span class="sd">				Node cached gradient. Usually is used to represent the gradient of the final output with respect to the input of this node, passing trough the path from final node to the current one.</span>
<span class="sd">			patch_type (str, default=&#39;zero&#39;): </span>
<span class="sd">				Type of intervention (&#39;zero&#39; or &#39;counterfactual&#39;). Zero patching corresponds to removing the message from the first node in the path to the input of the next node, while counterfactual patching corresponds to replacing the message with the counterfactual activation. In both cases the effect of the path is then calculated by propagating the message through the whole path.</span>
<span class="sd">			plot_patterns (bool, default=False):</span>
<span class="sd">				Whether to plot the attention patterns when calculating the forward pass. This is useful for debugging purposes but also to visualize the changes in the attention patterns when patching specific positions.</span>
<span class="sd">		</span>
<span class="sd">		Returns:</span>
<span class="sd">			self (ATTN_ApproxNode):</span>
<span class="sd">				The initialized ATTN_ApproxNode instance.</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span> <span class="n">parent</span><span class="o">=</span><span class="n">parent</span><span class="p">,</span> <span class="n">children</span><span class="o">=</span><span class="n">children</span><span class="p">,</span> <span class="n">msg_cache</span><span class="o">=</span><span class="n">msg_cache</span><span class="p">,</span> <span class="n">cf_cache</span><span class="o">=</span><span class="n">cf_cache</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient</span><span class="p">,</span> <span class="n">patch_type</span><span class="o">=</span><span class="n">patch_type</span><span class="p">,</span> <span class="n">input_name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;blocks.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">.hook_resid_pre&quot;</span><span class="p">,</span> <span class="n">output_name</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">head</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span> <span class="o">=</span> <span class="n">keyvalue_position</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">patch_key</span> <span class="o">=</span> <span class="n">patch_key</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">patch_value</span> <span class="o">=</span> <span class="n">patch_value</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">patch_query</span> <span class="o">=</span> <span class="n">patch_query</span>
		<span class="n">output_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;blocks.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">.head.</span><span class="si">{</span><span class="n">head</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="n">head</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="sa">f</span><span class="s2">&quot;blocks.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">&quot;</span>
		<span class="n">output_name</span> <span class="o">+=</span> <span class="s2">&quot;.hook_attn_out&quot;</span> <span class="k">if</span> <span class="n">keyvalue_position</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="sa">f</span><span class="s2">&quot;.kv.</span><span class="si">{</span><span class="n">keyvalue_position</span><span class="si">}</span><span class="s2">.hook_attn_out&quot;</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">output_name</span> <span class="o">=</span> <span class="n">output_name</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">attn_scores</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;blocks.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">.attn.hook_attn_scores&quot;</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">plot_patterns</span> <span class="o">=</span> <span class="n">plot_patterns</span>

		<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
			<span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="s2">&quot;query position must be greater than or equal to keyvalue position&quot;</span>
		<span class="k">if</span> <span class="n">msg_cache</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
			<span class="k">assert</span> <span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">output_name</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;Input and output shapes must match&quot;</span></div>


<div class="viewcode-block" id="ATTN_ApproxNode.forward">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.ATTN_ApproxNode.forward">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">message</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">		Calculate the effect of the message on the output of the node. </span>
<span class="sd">		</span>
<span class="sd">		The effect is calculated indirectly as the difference between the normal output of the component and the </span>
<span class="sd">		one obtained when the message is removed from the input of the node.</span>
<span class="sd">		On the other hand, if message is None the behavior depends on the patch_type:</span>
<span class="sd">		- &#39;zero&#39;: returns the normal output of the component</span>
<span class="sd">		- &#39;counterfactual&#39;: returns the difference between the normal output and the counterfactual output of the component</span>

<span class="sd">		Args:</span>

<span class="sd">			message (Tensor of shape (batch_size, seq_len, d_model), default=None):</span>
<span class="sd">				The message whose effect on the node need to be evaluated. If None, returns the normal </span>
<span class="sd">				output or the difference between normal and counterfactual output depending on patch_type.</span>
<span class="sd">			</span>
<span class="sd">		Returns:</span>
<span class="sd">			Tensor:</span>
<span class="sd">				A tensor representing the effect of the message on the output of the node.</span>
<span class="sd">				In simpler terms, it represents the message caused by passing the input message through this node.</span>

<span class="sd">		Notes:</span>
<span class="sd">		- If a position is specified the output will be zero for all other positions.</span>
<span class="sd">		- The method assumes that the msg_cache and cf_cache contain the necessary activations.</span>
<span class="sd">		- When message is None, the method will cache the output in msg_cache or cf_cache if not already present.</span>
<span class="sd">		- The method automatically adds entries to the msg_cache and cf_cache, correspoding to the output of single attention heads, if they are not already present.</span>
<span class="sd">		- This method uses precomputed attention scores if possible, this may introduce small numerical differences compared to a full recomputation.</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="n">length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="o">+</span><span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
		<span class="n">value_residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
		<span class="k">if</span> <span class="n">message</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
			<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_type</span> <span class="o">==</span> <span class="s1">&#39;zero&#39;</span><span class="p">:</span>
				<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">:</span>
					<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
						<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">output_name</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
					<span class="k">else</span><span class="p">:</span>
						<span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">])</span>
						<span class="n">out</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">output_name</span><span class="p">][:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
						<span class="k">return</span> <span class="n">out</span>
				<span class="k">else</span><span class="p">:</span>
					<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
						<span class="n">query_residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
					<span class="k">else</span><span class="p">:</span>
						<span class="n">query_residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">][:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
					<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
						<span class="n">key_residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">][:,</span> <span class="p">:</span><span class="n">length</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
					<span class="k">else</span><span class="p">:</span>
						<span class="n">key_residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">][:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
			<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_type</span> <span class="o">==</span> <span class="s1">&#39;counterfactual&#39;</span><span class="p">:</span>
				<span class="n">value_residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cf_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
				<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cf_cache</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">:</span>
					<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
						<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">output_name</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">cf_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">output_name</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
					<span class="k">else</span><span class="p">:</span>
						<span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">])</span>
						<span class="n">out</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">output_name</span><span class="p">][:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">cf_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">output_name</span><span class="p">][:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
						<span class="k">return</span> <span class="n">out</span>
				<span class="k">else</span><span class="p">:</span>
					<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
						<span class="n">query_residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cf_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
					<span class="k">else</span><span class="p">:</span>
						<span class="n">query_residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cf_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">][:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
					<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
						<span class="n">key_residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cf_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">][:,</span> <span class="p">:</span><span class="n">length</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
					<span class="k">else</span><span class="p">:</span>
						<span class="n">key_residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cf_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">][:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
		<span class="k">else</span><span class="p">:</span>
			<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_query</span><span class="p">:</span>
				<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
					<span class="n">query_residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="o">-</span> <span class="n">message</span>
				<span class="k">else</span><span class="p">:</span>
					<span class="n">query_residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">][:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="o">-</span> <span class="n">message</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span>
					<span class="n">query_residual</span> <span class="o">=</span> <span class="n">query_residual</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
			<span class="k">else</span><span class="p">:</span>
				<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
					<span class="n">query_residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
				<span class="k">else</span><span class="p">:</span>
					<span class="n">query_residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">][:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
			
			<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_key</span><span class="p">:</span>
				<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
					<span class="n">key_residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">][:,:</span><span class="n">length</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="o">-</span> <span class="n">message</span><span class="p">[:,:</span><span class="n">length</span><span class="p">]</span>
				<span class="k">else</span><span class="p">:</span>
					<span class="n">key_residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">][:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="o">-</span> <span class="n">message</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="p">:]</span>
					<span class="n">key_residual</span> <span class="o">=</span> <span class="n">key_residual</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
			<span class="k">else</span><span class="p">:</span>
				<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
					<span class="n">key_residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">][:,:</span><span class="n">length</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
				<span class="k">else</span><span class="p">:</span>
					<span class="n">key_residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">][:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
					<span class="n">key_residual</span> <span class="o">=</span> <span class="n">key_residual</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
			<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_value</span><span class="p">:</span>
				<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
					<span class="n">value_residual</span> <span class="o">=</span> <span class="n">value_residual</span> <span class="o">-</span> <span class="n">message</span>
				<span class="k">else</span><span class="p">:</span>
					<span class="n">value_residual</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">value_residual</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-</span> <span class="n">message</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="p">:]</span>

		<span class="n">key_residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">ln1</span><span class="p">(</span><span class="n">key_residual</span><span class="p">)</span>
		<span class="n">value_residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">ln1</span><span class="p">(</span><span class="n">value_residual</span><span class="p">)</span>
		<span class="n">query_residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">ln1</span><span class="p">(</span><span class="n">query_residual</span><span class="p">)</span>
		<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
			<span class="n">W_Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">W_Q</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
			<span class="n">W_K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">W_K</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
			<span class="n">W_V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">W_V</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
			<span class="n">b_Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">b_Q</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
			<span class="n">b_K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">b_K</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
			<span class="n">b_V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">b_V</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
			<span class="n">query</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bsd,ndh-&gt;bsnh&#39;</span><span class="p">,</span> <span class="n">query_residual</span><span class="p">,</span> <span class="n">W_Q</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_Q</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
			<span class="n">key</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bsd,ndh-&gt;bsnh&#39;</span><span class="p">,</span> <span class="n">key_residual</span><span class="p">,</span> <span class="n">W_K</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_K</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
			<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
				<span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bd,ndh-&gt;bnh&#39;</span><span class="p">,</span> <span class="n">value_residual</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="p">:],</span> <span class="n">W_V</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_V</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
				<span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">value_residual</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">v</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
				<span class="n">value</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">v</span>
			<span class="k">else</span><span class="p">:</span>
				<span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bsd,ndh-&gt;bsnh&#39;</span><span class="p">,</span> <span class="n">value_residual</span><span class="p">,</span> <span class="n">W_V</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_V</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
		<span class="k">else</span><span class="p">:</span>
			<span class="n">W_Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">W_Q</span>
			<span class="n">W_K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">W_K</span>
			<span class="n">W_V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">W_V</span>
			<span class="n">b_Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">b_Q</span>
			<span class="n">b_K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">b_K</span>
			<span class="n">b_V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">b_V</span>
			<span class="n">query</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bsd,ndh-&gt;bsnh&#39;</span><span class="p">,</span> <span class="n">query_residual</span><span class="p">,</span> <span class="n">W_Q</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_Q</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
			<span class="n">key</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bsd,ndh-&gt;bsnh&#39;</span><span class="p">,</span> <span class="n">key_residual</span><span class="p">,</span> <span class="n">W_K</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_K</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
			<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
				<span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bd,ndh-&gt;bnh&#39;</span><span class="p">,</span> <span class="n">value_residual</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="p">:],</span> <span class="n">W_V</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_V</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
				<span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">value_residual</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">v</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
				<span class="n">value</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">v</span>
			<span class="k">else</span><span class="p">:</span>
				<span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bsd,ndh-&gt;bsnh&#39;</span><span class="p">,</span> <span class="n">value_residual</span><span class="p">,</span> <span class="n">W_V</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_V</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
		<span class="n">out</span> <span class="o">=</span> <span class="n">custom_attention_forward</span><span class="p">(</span>
			<span class="n">attention_module</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="p">,</span>
			<span class="n">head</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">,</span>
			<span class="n">q</span><span class="o">=</span><span class="n">query</span><span class="p">,</span>
			<span class="n">k</span><span class="o">=</span><span class="n">key</span><span class="p">,</span>
			<span class="n">v</span><span class="o">=</span><span class="n">value</span><span class="p">,</span>
			<span class="n">precomputed_attention_scores</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attn_scores</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span>
			<span class="n">query_position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span>
			<span class="n">keyvalue_position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span>
			<span class="n">plot_patterns</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">plot_patterns</span>
		<span class="p">)</span>
		

		<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
			<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">message</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
				<span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">output_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
			<span class="k">else</span><span class="p">:</span>
				<span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">,</span> <span class="n">msg_cache</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">,</span> <span class="n">cf_cache</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cf_cache</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="n">patch_type</span><span class="o">=</span><span class="s1">&#39;zero&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">message</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
		<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_type</span> <span class="o">==</span> <span class="s1">&#39;counterfactual&#39;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">cf_cache</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
			<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">message</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
				<span class="bp">self</span><span class="o">.</span><span class="n">cf_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">output_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
			<span class="k">else</span><span class="p">:</span>
				<span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">,</span> <span class="n">msg_cache</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cf_cache</span><span class="p">,</span> <span class="n">cf_cache</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cf_cache</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="n">patch_type</span><span class="o">=</span><span class="s1">&#39;counterfactual&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">message</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
		<span class="k">if</span> <span class="n">message</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
			<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_type</span> <span class="o">==</span> <span class="s1">&#39;zero&#39;</span><span class="p">:</span>
				<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
					<span class="n">resized_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">out</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
					<span class="n">resized_out</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">out</span>
					<span class="k">return</span> <span class="n">resized_out</span>
				<span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">output_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
				<span class="k">return</span> <span class="n">out</span>
			<span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_type</span> <span class="o">==</span> <span class="s1">&#39;counterfactual&#39;</span><span class="p">:</span>
				<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
					<span class="n">resized_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">out</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
					<span class="n">resized_out</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">output_name</span><span class="p">][:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="o">-</span> <span class="n">out</span>
					<span class="k">return</span> <span class="n">resized_out</span>
				<span class="bp">self</span><span class="o">.</span><span class="n">cf_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">output_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
				<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">output_name</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">cf_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">output_name</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
			<span class="k">else</span><span class="p">:</span>
				<span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown patch type: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">patch_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
		
		<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
			<span class="n">resized_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">out</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
			<span class="n">resized_out</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">output_name</span><span class="p">][:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="o">-</span> <span class="n">out</span>
			<span class="k">return</span> <span class="n">resized_out</span>
		<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">output_name</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="o">-</span> <span class="n">out</span></div>

	
<div class="viewcode-block" id="ATTN_ApproxNode.calculate_gradient">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.ATTN_ApproxNode.calculate_gradient">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="nf">calculate_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_precomputed</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">		Calculates the gradient of the node&#39;s input with respect to the final output.</span>
<span class="sd">		By default the gradient is calculated propagating backwards from the parent node if present,</span>
<span class="sd">		or assuming a gradient of ones if self has no parent. When &#39;grad_outputs&#39; is specified, it is used instead of the parent&#39;s gradient.</span>

<span class="sd">		Args:</span>
<span class="sd">			grad_outputs : Tensor, optional (default=None)</span>
<span class="sd">				Gradient to propagate backwards. If None, uses the gradient from the parent node or ones.</span>
<span class="sd">			</span>
<span class="sd">			save : bool, optional (default=True)</span>
<span class="sd">				Whether to save the computed gradient in self.gradient. The gradient can be reused</span>
<span class="sd">				later by setting use_precomputed to True.</span>
<span class="sd">			</span>
<span class="sd">			use_precomputed : bool, optional (default=False)</span>
<span class="sd">				Whether to use the precomputed gradient if available. The precoputed gradient is stored whenever</span>
<span class="sd">				save is True.</span>
<span class="sd">		</span>
<span class="sd">		Returns:</span>
<span class="sd">			gradient : Tensor</span>
<span class="sd">				A tensor representing the gradient of the output with respect to the input</span>
<span class="sd">				of this node, passing trough the path from final node to the current one.</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">use_precomputed</span><span class="p">:</span>
			<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
				<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
			<span class="n">gradient</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
			<span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">gradient</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">gradient</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
			<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_query</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_key</span><span class="p">:</span>
				<span class="n">out</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="p">:]</span>
			<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_query</span><span class="p">:</span>
				<span class="n">out</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span>
			<span class="k">return</span> <span class="n">out</span>
		<span class="n">input_residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
		<span class="n">input_residual</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

		<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
			<span class="n">length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="o">+</span><span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
			
			<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
				<span class="n">query_residual</span> <span class="o">=</span> <span class="n">input_residual</span>
			<span class="k">else</span><span class="p">:</span>
				<span class="n">query_residual</span> <span class="o">=</span> <span class="n">input_residual</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
			<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
				<span class="n">key_residual</span> <span class="o">=</span> <span class="n">input_residual</span><span class="p">[:,</span> <span class="p">:</span><span class="n">length</span><span class="p">]</span>
			<span class="k">else</span><span class="p">:</span>
				<span class="n">key_residual</span> <span class="o">=</span> <span class="n">input_residual</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
			<span class="n">value_residual</span> <span class="o">=</span> <span class="n">input_residual</span>
			<span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_query</span><span class="p">:</span>
				<span class="n">query_residual</span> <span class="o">=</span> <span class="n">query_residual</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="c1"># detach from gradient computation</span>
			<span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_key</span><span class="p">:</span>
				<span class="n">key_residual</span> <span class="o">=</span> <span class="n">key_residual</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="c1"># detach from gradient computation</span>
			<span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_value</span><span class="p">:</span>
				<span class="n">value_residual</span> <span class="o">=</span> <span class="n">value_residual</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="c1"># detach from gradient computation</span>
			<span class="n">key_residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">ln1</span><span class="p">(</span><span class="n">key_residual</span><span class="p">)</span>
			<span class="n">value_residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">ln1</span><span class="p">(</span><span class="n">value_residual</span><span class="p">)</span>
			<span class="n">query_residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">ln1</span><span class="p">(</span><span class="n">query_residual</span><span class="p">)</span>
			<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
				<span class="n">W_Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">W_Q</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
				<span class="n">W_K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">W_K</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
				<span class="n">W_V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">W_V</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
				<span class="n">b_Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">b_Q</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
				<span class="n">b_K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">b_K</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
				<span class="n">b_V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">b_V</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
				<span class="n">query</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bsd,ndh-&gt;bsnh&#39;</span><span class="p">,</span> <span class="n">query_residual</span><span class="p">,</span> <span class="n">W_Q</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_Q</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
				<span class="n">key</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bsd,ndh-&gt;bsnh&#39;</span><span class="p">,</span> <span class="n">key_residual</span><span class="p">,</span> <span class="n">W_K</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_K</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
				<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
					<span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bd,ndh-&gt;bnh&#39;</span><span class="p">,</span> <span class="n">value_residual</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="p">:],</span> <span class="n">W_V</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_V</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
					<span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">value_residual</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">v</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
					<span class="n">value</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">v</span>
				<span class="k">else</span><span class="p">:</span>
					<span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bsd,ndh-&gt;bsnh&#39;</span><span class="p">,</span> <span class="n">value_residual</span><span class="p">,</span> <span class="n">W_V</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_V</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
			<span class="k">else</span><span class="p">:</span>
				<span class="n">W_Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">W_Q</span>
				<span class="n">W_K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">W_K</span>
				<span class="n">W_V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">W_V</span>
				<span class="n">b_Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">b_Q</span>
				<span class="n">b_K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">b_K</span>
				<span class="n">b_V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">b_V</span>
				<span class="n">query</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bsd,ndh-&gt;bsnh&#39;</span><span class="p">,</span> <span class="n">query_residual</span><span class="p">,</span> <span class="n">W_Q</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_Q</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
				<span class="n">key</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bsd,ndh-&gt;bsnh&#39;</span><span class="p">,</span> <span class="n">key_residual</span><span class="p">,</span> <span class="n">W_K</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_K</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
				<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
					<span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bd,ndh-&gt;bnh&#39;</span><span class="p">,</span> <span class="n">value_residual</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="p">:],</span> <span class="n">W_V</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_V</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
					<span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">value_residual</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">v</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
					<span class="n">value</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">v</span>
				<span class="k">else</span><span class="p">:</span>
					<span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bsd,ndh-&gt;bsnh&#39;</span><span class="p">,</span> <span class="n">value_residual</span><span class="p">,</span> <span class="n">W_V</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_V</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
			<span class="n">out</span> <span class="o">=</span> <span class="n">custom_attention_forward</span><span class="p">(</span>
				<span class="n">attention_module</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="p">,</span>
				<span class="n">head</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">,</span>
				<span class="n">q</span><span class="o">=</span><span class="n">query</span><span class="p">,</span>
				<span class="n">k</span><span class="o">=</span><span class="n">key</span><span class="p">,</span>
				<span class="n">v</span><span class="o">=</span><span class="n">value</span><span class="p">,</span>
				<span class="n">precomputed_attention_scores</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attn_scores</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span>
				<span class="n">query_position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span>
				<span class="n">keyvalue_position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span>
				<span class="n">plot_patterns</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">plot_patterns</span>
			<span class="p">)</span>
			<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
				<span class="n">resized_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">out</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
				<span class="n">resized_out</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">out</span>
			<span class="k">else</span><span class="p">:</span>
				<span class="n">resized_out</span> <span class="o">=</span> <span class="n">out</span>

		<span class="k">if</span> <span class="n">grad_outputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
			<span class="n">grad_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">calculate_gradient</span><span class="p">(</span><span class="n">save</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_precomputed</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">input_residual</span><span class="p">)</span>
		<span class="n">gradient</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span>
			<span class="n">resized_out</span><span class="p">,</span>
			<span class="n">input_residual</span><span class="p">,</span>
			<span class="n">grad_outputs</span><span class="o">=</span><span class="n">grad_outputs</span><span class="p">,</span>
			<span class="n">allow_unused</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
		<span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
		<span class="k">if</span> <span class="n">save</span><span class="p">:</span>
			<span class="bp">self</span><span class="o">.</span><span class="n">gradient</span> <span class="o">=</span> <span class="n">gradient</span>
		<span class="k">return</span> <span class="n">gradient</span></div>



<div class="viewcode-block" id="ATTN_ApproxNode.get_expansion_candidates">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.ATTN_ApproxNode.get_expansion_candidates">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="nf">get_expansion_candidates</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_cfg</span><span class="p">:</span> <span class="n">HookedTransformerConfig</span><span class="p">,</span> <span class="n">include_head</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">separate_kv</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">ApproxNode</span><span class="p">]:</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">		Returns the list of predecessors nodes in the computational graph whose outputs influence the</span>
<span class="sd">		output of this node. </span>
<span class="sd">		Previous nodes are:</span>
<span class="sd">			- MLP, EMBED and ATTN nodes in self.position from previous layers if patch_query=True.</span>
<span class="sd">			- MLP, EMBED and ATTN nodes in all previous positions from previous layers if patch_key=True or patch_value=True.</span>
<span class="sd">		Args:</span>

<span class="sd">			model_cfg (HookedTransformerConfig):</span>
<span class="sd">				The configuration of the transformer model. It is used to determine the number of heads and other model parameters.</span>

<span class="sd">			include_head (bool, default=False):</span>
<span class="sd">				Whether to consider specific head nodes for ATTN.</span>

<span class="sd">			separate_kv (bool, default=False):</span>
<span class="sd">				Whether to consider key and value positions separately for ATTN nodes </span>
<span class="sd">		</span>
<span class="sd">		Returns:</span>
<span class="sd">			list of ApproxNode:</span>
<span class="sd">				The list of all predecessor nodes infuencing the input of this node.</span>

<span class="sd">		Notes:</span>
<span class="sd">			- If self.position is None, only non-position-specific previous nodes are considered.</span>
<span class="sd">		&quot;&quot;&quot;</span>	
		<span class="n">prev_nodes</span> <span class="o">=</span> <span class="p">[]</span>
		<span class="n">common_args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;msg_cache&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">,</span> <span class="s2">&quot;parent&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;patch_type&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_type</span><span class="p">,</span> <span class="s2">&quot;cf_cache&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">cf_cache</span><span class="p">}</span>

		<span class="c1"># MLPs</span>
		<span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">):</span>
			<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_query</span><span class="p">:</span>
				<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">MLP_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>
			<span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">patch_key</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_value</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_query</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">):</span>
				<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">MLP_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>

		<span class="c1"># EMBED node</span>
		<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_query</span><span class="p">:</span>
			<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">EMBED_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>
		<span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">patch_key</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_value</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_query</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">):</span>
			<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">EMBED_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>

		<span class="c1"># ATTN nodes patching current query position</span>
		<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_query</span><span class="p">:</span>
			<span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">):</span>
				<span class="c1"># prev ATTN query positions</span>
				<span class="k">if</span> <span class="n">include_head</span><span class="p">:</span>
					<span class="n">prev_nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="n">patch_key</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model_cfg</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)])</span>
				<span class="k">else</span><span class="p">:</span>
					<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="n">patch_key</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>

				<span class="c1"># prev ATTN key-value positions</span>
				<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
					<span class="k">for</span> <span class="n">keyvalue_position</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
						<span class="k">if</span> <span class="n">include_head</span><span class="p">:</span>
							<span class="k">if</span> <span class="n">separate_kv</span><span class="p">:</span>
								<span class="n">prev_nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model_cfg</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)])</span>
								<span class="n">prev_nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model_cfg</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)])</span>
							<span class="k">else</span><span class="p">:</span>
								<span class="n">prev_nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model_cfg</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)])</span>
						<span class="k">else</span><span class="p">:</span>
							<span class="k">if</span> <span class="n">separate_kv</span><span class="p">:</span>
								<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>
								<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>
							<span class="k">else</span><span class="p">:</span>
								<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>
				<span class="k">else</span><span class="p">:</span>
					<span class="k">if</span> <span class="n">include_head</span><span class="p">:</span>
						<span class="k">if</span> <span class="n">separate_kv</span><span class="p">:</span>
							<span class="n">prev_nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model_cfg</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)])</span>
							<span class="n">prev_nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model_cfg</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)])</span>
						<span class="k">else</span><span class="p">:</span>
							<span class="n">prev_nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model_cfg</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)])</span>
					<span class="k">else</span><span class="p">:</span>
						<span class="k">if</span> <span class="n">separate_kv</span><span class="p">:</span>
							<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>
							<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>
						<span class="k">else</span><span class="p">:</span>
							<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>

		<span class="c1"># ATTN nodes patching current key-value position</span>
		<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_key</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_value</span><span class="p">:</span>
			<span class="n">keyvalue_positions</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span>
			<span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">):</span>
				<span class="c1"># prev ATTN query positions</span>
				<span class="k">if</span> <span class="n">include_head</span><span class="p">:</span>
					<span class="n">prev_nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="n">patch_key</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model_cfg</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)])</span>
				<span class="k">else</span><span class="p">:</span>
					<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="n">patch_key</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>

				<span class="c1"># prev ATTN key-value positions</span>
				<span class="k">for</span> <span class="n">prev_keyvalue_position</span> <span class="ow">in</span> <span class="n">keyvalue_positions</span><span class="p">:</span>
					<span class="k">if</span> <span class="n">include_head</span><span class="p">:</span>
						<span class="k">if</span> <span class="n">separate_kv</span><span class="p">:</span>
							<span class="n">prev_nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="n">prev_keyvalue_position</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model_cfg</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)])</span>
							<span class="n">prev_nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="n">prev_keyvalue_position</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model_cfg</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)])</span>
						<span class="k">else</span><span class="p">:</span>
							<span class="n">prev_nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="n">prev_keyvalue_position</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model_cfg</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)])</span>
					<span class="k">else</span><span class="p">:</span>
						<span class="k">if</span> <span class="n">separate_kv</span><span class="p">:</span>
							<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="n">prev_keyvalue_position</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>
							<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="n">prev_keyvalue_position</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>
						<span class="k">else</span><span class="p">:</span>
							<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="n">prev_keyvalue_position</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>

		<span class="c1"># Remove duplicates</span>
		<span class="n">prev_nodes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">prev_nodes</span><span class="p">))</span>
		<span class="k">return</span> <span class="n">prev_nodes</span></div>


<div class="viewcode-block" id="ATTN_ApproxNode.__repr__">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.ATTN_ApproxNode.__repr__">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;String representation of the ATTN_ApproxNode instance.</span>
<span class="sd">		Includes layer, head, position, keyvalue_position, and patching options.</span>
<span class="sd">		</span>
<span class="sd">		Returns:</span>
<span class="sd">			str:</span>
<span class="sd">				A string representation of the ATTN_ApproxNode instance.</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;ATTN_ApproxNode(layer=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="si">}</span><span class="s2">, head=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="si">}</span><span class="s2">, position=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="si">}</span><span class="s2">, keyvalue_position=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="si">}</span><span class="s2">, patch_query=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">patch_query</span><span class="si">}</span><span class="s2">, patch_key=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">patch_key</span><span class="si">}</span><span class="s2">, patch_value=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">patch_value</span><span class="si">}</span><span class="s2">)&quot;</span></div>


<div class="viewcode-block" id="ATTN_ApproxNode.__hash__">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.ATTN_ApproxNode.__hash__">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="fm">__hash__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;Hash function for the ATTN_ApproxNode instance.</span>
<span class="sd">		</span>
<span class="sd">		Returns:</span>
<span class="sd">			int:</span>
<span class="sd">				A hash value based on the layer, head, position, keyvalue_position, and patching options.</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">return</span> <span class="nb">hash</span><span class="p">((</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_query</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_key</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_value</span><span class="p">))</span></div>
</div>



<div class="viewcode-block" id="EMBED_ApproxNode">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.EMBED_ApproxNode">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">EMBED_ApproxNode</span><span class="p">(</span><span class="n">ApproxNode</span><span class="p">):</span>
<span class="w">	</span><span class="sd">&quot;&quot;&quot;Represents the embedding node in the transformer. This is almost a dummy node, as it only serves as the starting point for paths that begin at the input embeddings. This classes uses cached activations from the model to provide an interface consistent with other node types.	</span>

<span class="sd">	Attributes:</span>
<span class="sd">		model (HookedTransformer): </span>
<span class="sd">			The transformer model instance. It is assumed to be a HookedTransformer from transformer_lens library. Any other implementation which provide the same interface should work as well.</span>
<span class="sd">		layer (int): </span>
<span class="sd">			Layer index in the transformer. Embedding layer is assumed to be layer 0.</span>
<span class="sd">		position (int, default=None): </span>
<span class="sd">			Token position if position-specific, else None. None is equivalent to all positions.</span>
<span class="sd">		parent (ApproxNode, default=None): </span>
<span class="sd">			Parent node in the next node in the path. The parent is a successor in the computational graph.</span>
<span class="sd">		children (set, default=set()): </span>
<span class="sd">			Set of child nodes. A child is a predecessor in the computational graph.</span>
<span class="sd">		msg_cache (dict): </span>
<span class="sd">			Clean activation cache. Can be obtained by running the model with hooks using the clean prompt and converting the result to a dictionary. It must be a dictionary because it might be modified by adding new cached entries, corresponding to the outputs of subcomponents (e.g. single attention heads). </span>
<span class="sd">		cf_cache (dict, default={}): </span>
<span class="sd">			Counterfactual activation cache. Can be obtained by running the model with hooks using the corrupted prompt and converting the result to a dictionary. It must be a dictionary because it might be modified by adding new cached entries, corresponding to the outputs of subcomponents (e.g. single attention heads).</span>
<span class="sd">		gradient (Tensor, default=None): </span>
<span class="sd">			Node cached gradient. Usually is used to represent the gradient of the final output with respect to the input of this node, passing trough the path from final node to the current one.</span>
<span class="sd">		input_name (str): </span>
<span class="sd">			Input activation name. This is the name associated to the cache entry corresponding to the input of this node.</span>
<span class="sd">		output_name (str): </span>
<span class="sd">			Output activation name. This is the name associated to the cache entry corresponding to the output of this node.</span>
<span class="sd">		patch_type (str, default=&#39;zero&#39;): </span>
<span class="sd">			Type of intervention (&#39;zero&#39; or &#39;counterfactual&#39;). Zero patching corresponds to removing the message from the first node in the path to the input of the next node, while counterfactual patching corresponds to replacing the message with the counterfactual activation. In both cases the effect of the path is then calculated by propagating the message through the whole path.</span>
<span class="sd">	&quot;&quot;&quot;</span>
<div class="viewcode-block" id="EMBED_ApproxNode.__init__">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.EMBED_ApproxNode.__init__">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">HookedTransformer</span><span class="p">,</span> <span class="n">layer</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">position</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">parent</span><span class="p">:</span> <span class="n">ApproxNode</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">children</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(),</span> <span class="n">msg_cache</span> <span class="o">=</span> <span class="p">{},</span> <span class="n">cf_cache</span> <span class="o">=</span> <span class="p">{},</span> <span class="n">gradient</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">patch_type</span> <span class="o">=</span> <span class="s1">&#39;zero&#39;</span><span class="p">):</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;Initializes the EMBED_ApproxNode instance.</span>
<span class="sd">		Args:</span>
<span class="sd">			model (HookedTransformer): </span>
<span class="sd">				The transformer model instance. It is assumed to be a HookedTransformer from transformer_lens library. Any other implementation which provide the same interface should work as well.</span>
<span class="sd">			layer (int): </span>
<span class="sd">				Layer index in the transformer. Embedding layer is assumed to be layer 0.</span>
<span class="sd">			position (int, default=None): </span>
<span class="sd">				Token position if position-specific, else None. None is equivalent to all positions.</span>
<span class="sd">			parent (ApproxNode, default=None): </span>
<span class="sd">				Parent node in the next node in the path. The parent is a successor in the computational graph.</span>
<span class="sd">			children (set, default=set()): </span>
<span class="sd">				Set of child nodes. A child is a predecessor in the computational graph.</span>
<span class="sd">			msg_cache (dict): </span>
<span class="sd">				Clean activation cache. Can be obtained by running the model with hooks using the clean prompt and converting the result to a dictionary. It must be a dictionary because it might be modified by adding new cached entries, corresponding to the outputs of subcomponents (e.g. single attention heads). </span>
<span class="sd">			cf_cache (dict, default={}): </span>
<span class="sd">				Counterfactual activation cache. Can be obtained by running the model with hooks using the corrupted prompt and converting the result to a dictionary. It must be a dictionary because it might be modified by adding new cached entries, corresponding to the outputs of subcomponents (e.g. single attention heads).</span>
<span class="sd">			gradient (Tensor, default=None): </span>
<span class="sd">				Node cached gradient. Usually is used to represent the gradient of the final output with respect to the input of this node, passing trough the path from final node to the current one.</span>
<span class="sd">			patch_type (str, default=&#39;zero&#39;): </span>
<span class="sd">				Type of intervention (&#39;zero&#39; or &#39;counterfactual&#39;). Zero patching corresponds to removing the message from the first node in the path to the input of the next node, while counterfactual patching corresponds to replacing the message with the counterfactual activation. In both cases the effect of the path is then calculated by propagating the message through the whole path.</span>
<span class="sd">		Returns:</span>
<span class="sd">			self (EMBED_ApproxNode):</span>
<span class="sd">				An instance of the EMBED_ApproxNode class.</span>
<span class="sd">		&quot;&quot;&quot;</span>		
		<span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span> <span class="n">parent</span><span class="o">=</span><span class="n">parent</span><span class="p">,</span> <span class="n">children</span><span class="o">=</span><span class="n">children</span><span class="p">,</span> <span class="n">msg_cache</span><span class="o">=</span><span class="n">msg_cache</span><span class="p">,</span> <span class="n">cf_cache</span><span class="o">=</span><span class="n">cf_cache</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient</span><span class="p">,</span> <span class="n">input_name</span><span class="o">=</span><span class="s2">&quot;hook_embed&quot;</span><span class="p">,</span> <span class="n">output_name</span><span class="o">=</span><span class="s2">&quot;hook_embed&quot;</span><span class="p">,</span> <span class="n">patch_type</span><span class="o">=</span><span class="n">patch_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="EMBED_ApproxNode.forward">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.EMBED_ApproxNode.forward">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">message</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">		Calculate the effect of the message on the output of the node. </span>
<span class="sd">		</span>
<span class="sd">		The effect is calculated indirectly as the difference between the normal output of the component and the </span>
<span class="sd">		one obtained when the message is removed from the input of the node.</span>
<span class="sd">		On the other hand, if message is None the behavior depends on the patch_type:</span>
<span class="sd">		- &#39;zero&#39;: returns the normal output of the component</span>
<span class="sd">		- &#39;counterfactual&#39;: returns the difference between the normal output and the counterfactual output of the component</span>

<span class="sd">		Args:</span>

<span class="sd">			message (Tensor of shape (batch_size, seq_len, d_model), default=None):</span>
<span class="sd">				The message whose effect on the node need to be evaluated. If None, returns the normal </span>
<span class="sd">				output or the difference between normal and counterfactual output depending on patch_type.</span>
<span class="sd">			</span>
<span class="sd">		Returns:</span>
<span class="sd">			Tensor:</span>
<span class="sd">				A tensor representing the effect of the message on the output of the node.</span>
<span class="sd">				In simpler terms, it represents the message caused by passing the input message through this node.</span>

<span class="sd">		Notes:</span>
<span class="sd">			- If a position is specified the output will be zero for all other positions.</span>
<span class="sd">			- The method assumes that the msg_cache and cf_cache contain the necessary activations.</span>
<span class="sd">			- When message is None, the method will cache the output in msg_cache or cf_cache if not already present.</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">if</span> <span class="n">message</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
			<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_type</span> <span class="o">==</span> <span class="s1">&#39;zero&#39;</span><span class="p">:</span>
				<span class="n">embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="s2">&quot;hook_embed&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
			<span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_type</span> <span class="o">==</span> <span class="s1">&#39;counterfactual&#39;</span><span class="p">:</span>
				<span class="n">embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="s2">&quot;hook_embed&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">cf_cache</span><span class="p">[</span><span class="s2">&quot;hook_embed&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
			<span class="k">else</span><span class="p">:</span>
				<span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown patch type: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">patch_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
		<span class="k">else</span><span class="p">:</span>
			<span class="n">embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="s2">&quot;hook_embed&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="o">-</span> <span class="n">message</span>
		<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
			<span class="n">embedding</span><span class="p">[:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">embedding</span><span class="p">[:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:],</span> <span class="n">device</span><span class="o">=</span><span class="n">embedding</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
			<span class="n">embedding</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">embedding</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:,</span> <span class="p">:],</span> <span class="n">device</span><span class="o">=</span><span class="n">embedding</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
		<span class="k">return</span> <span class="n">embedding</span></div>


<div class="viewcode-block" id="EMBED_ApproxNode.calculate_gradient">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.EMBED_ApproxNode.calculate_gradient">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="nf">calculate_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_precomputed</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">		Calculates the gradient of the node&#39;s input with respect to the final output.</span>
<span class="sd">		By default the gradient is calculated propagating backwards from the parent node if present,</span>
<span class="sd">		or assuming a gradient of ones if self has no parent. When &#39;grad_outputs&#39; is specified, it is used instead of the parent&#39;s gradient.</span>

<span class="sd">		Args:</span>
<span class="sd">			grad_outputs : Tensor, optional (default=None)</span>
<span class="sd">				Usually the gradient to propagate backwards in this particular case it is never used.</span>
<span class="sd">			</span>
<span class="sd">			save : bool, optional (default=True)</span>
<span class="sd">				Whether to save the computed gradient in self.gradient. The gradient can be reused</span>
<span class="sd">				later by setting use_precomputed to True.</span>
<span class="sd">			</span>
<span class="sd">			use_precomputed : bool, optional (default=False)</span>
<span class="sd">				Whether to use the precomputed gradient if available. The precoputed gradient is stored whenever</span>
<span class="sd">				save is True.</span>
<span class="sd">		</span>
<span class="sd">		Returns:</span>
<span class="sd">			gradient : Tensor</span>
<span class="sd">				A tensor representing the gradient of the output with respect to the input</span>
<span class="sd">				of this node, passing trough the path from final node to the current one.</span>
<span class="sd">		</span>
<span class="sd">		Notes:</span>
<span class="sd">			- Given that the EMBED node is a dummy node, the gradient is simply the one provided or the one from the parent node. The only modification is to zero out the gradient for positions not equal to self.position if specified.</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">use_precomputed</span><span class="p">:</span>
			<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
				<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
			<span class="n">gradient</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
			<span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">gradient</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
			<span class="n">out</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span>
			<span class="k">return</span> <span class="n">out</span>
		<span class="k">if</span> <span class="n">grad_outputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
			<span class="n">gradient</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">calculate_gradient</span><span class="p">(</span><span class="n">grad_outputs</span><span class="p">,</span> <span class="n">save</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_precomputed</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">])</span>
		<span class="k">else</span><span class="p">:</span>
			<span class="n">gradient</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">calculate_gradient</span><span class="p">(</span><span class="n">grad_outputs</span><span class="p">,</span> <span class="n">save</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_precomputed</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">])</span>
		<span class="n">gradient</span><span class="p">[:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">gradient</span><span class="p">[:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:],</span> <span class="n">device</span><span class="o">=</span><span class="n">gradient</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
		<span class="n">gradient</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">gradient</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:,</span> <span class="p">:],</span> <span class="n">device</span><span class="o">=</span><span class="n">gradient</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
		<span class="k">if</span> <span class="n">save</span><span class="p">:</span>
			<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
				<span class="bp">self</span><span class="o">.</span><span class="n">gradient</span> <span class="o">=</span> <span class="n">gradient</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
			<span class="k">else</span><span class="p">:</span>
				<span class="bp">self</span><span class="o">.</span><span class="n">gradient</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
		<span class="k">return</span> <span class="n">gradient</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span></div>



<div class="viewcode-block" id="EMBED_ApproxNode.get_expansion_candidates">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.EMBED_ApproxNode.get_expansion_candidates">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="nf">get_expansion_candidates</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_cfg</span><span class="p">:</span> <span class="n">HookedTransformerConfig</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">include_head</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">separate_kv</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">ApproxNode</span><span class="p">]:</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">		Returns the list of predecessors nodes in the computational graph whose outputs influence the</span>
<span class="sd">		output of this node. </span>
<span class="sd">		Given that this is an EMBED node, there are no predecessors, so the method returns an empty list.</span>
<span class="sd">		Args:</span>

<span class="sd">			model_cfg (HookedTransformerConfig):</span>
<span class="sd">				The configuration of the transformer model. It is used to determine the number of heads and other model parameters.</span>

<span class="sd">			include_head (bool, default=False):</span>
<span class="sd">				Whether to consider specific head nodes for ATTN.</span>

<span class="sd">			separate_kv (bool, default=False):</span>
<span class="sd">				Whether to consider key and value positions separately for ATTN nodes </span>
<span class="sd">		</span>
<span class="sd">		Returns:</span>
<span class="sd">			list of ApproxNode:</span>
<span class="sd">				The list of all predecessor, which is always empty for EMBED nodes.</span>
<span class="sd">		&quot;&quot;&quot;</span>	
		<span class="k">return</span> <span class="p">[]</span></div>


<div class="viewcode-block" id="EMBED_ApproxNode.__repr__">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.EMBED_ApproxNode.__repr__">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;String representation of the EMBED_ApproxNode instance.</span>
<span class="sd">		Includes layer and position.</span>

<span class="sd">		Returns:</span>
<span class="sd">			str:</span>
<span class="sd">				A string representation of the EMBED_ApproxNode instance.</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;EMBED_ApproxNode(layer=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="si">}</span><span class="s2">, position=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="si">}</span><span class="s2">)&quot;</span></div>


<div class="viewcode-block" id="EMBED_ApproxNode.__hash__">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.EMBED_ApproxNode.__hash__">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="fm">__hash__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;Hash function for the EMBED_ApproxNode instance.</span>
<span class="sd">		Returns:</span>
<span class="sd">			int:</span>
<span class="sd">				A hash value based on the layer and position.</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">return</span> <span class="nb">hash</span><span class="p">((</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">))</span></div>
</div>



<div class="viewcode-block" id="FINAL_ApproxNode">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.FINAL_ApproxNode">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">FINAL_ApproxNode</span><span class="p">(</span><span class="n">ApproxNode</span><span class="p">):</span>
<span class="w">	</span><span class="sd">&quot;&quot;&quot;Represents the final node in the transformer. This is almost a dummy node, as it only serves as the final point for paths that begin at the input embeddings. This classes uses cached activations from the model to provide an interface consistent with other node types.</span>

<span class="sd">	Attributes:</span>
<span class="sd">		model (HookedTransformer): </span>
<span class="sd">			The transformer model instance. It is assumed to be a HookedTransformer from transformer_lens library. Any other implementation which provide the same interface should work as well.</span>
<span class="sd">		layer (int): </span>
<span class="sd">			Layer index in the transformer. Embedding layer is assumed to be layer 0.</span>
<span class="sd">		position (int, default=None): </span>
<span class="sd">			Token position if position-specific, else None. None is equivalent to all positions.</span>
<span class="sd">		parent (ApproxNode, default=None): </span>
<span class="sd">			Parent node in the next node in the path. The parent is a successor in the computational graph.</span>
<span class="sd">		children (set, default=set()): </span>
<span class="sd">			Set of child nodes. A child is a predecessor in the computational graph.</span>
<span class="sd">		msg_cache (dict): </span>
<span class="sd">			Clean activation cache. Can be obtained by running the model with hooks using the clean prompt and converting the result to a dictionary. It must be a dictionary because it might be modified by adding new cached entries, corresponding to the outputs of subcomponents (e.g. single attention heads). </span>
<span class="sd">		cf_cache (dict, default={}): </span>
<span class="sd">			Counterfactual activation cache. Can be obtained by running the model with hooks using the corrupted prompt and converting the result to a dictionary. It must be a dictionary because it might be modified by adding new cached entries, corresponding to the outputs of subcomponents (e.g. single attention heads).</span>
<span class="sd">		gradient (Tensor, default=None): </span>
<span class="sd">			Node cached gradient. Usually is used to represent the gradient of the final output with respect to the input of this node, passing trough the path from final node to the current one.</span>
<span class="sd">		input_name (str): </span>
<span class="sd">			Input activation name. This is the name associated to the cache entry corresponding to the input of this node.</span>
<span class="sd">		output_name (str): </span>
<span class="sd">			Output activation name. This is the name associated to the cache entry corresponding to the output of this node.</span>
<span class="sd">		patch_type (str, default=&#39;zero&#39;): </span>
<span class="sd">			Type of intervention (&#39;zero&#39; or &#39;counterfactual&#39;). Zero patching corresponds to removing the message from the first node in the path to the input of the next node, while counterfactual patching corresponds to replacing the message with the counterfactual activation. In both cases the effect of the path is then calculated by propagating the message through the whole path.</span>
<span class="sd">	&quot;&quot;&quot;</span>
<div class="viewcode-block" id="FINAL_ApproxNode.__init__">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.FINAL_ApproxNode.__init__">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">HookedTransformer</span><span class="p">,</span> <span class="n">layer</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">metric</span><span class="p">:</span> <span class="nb">callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">parent</span><span class="p">:</span> <span class="n">ApproxNode</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">children</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(),</span> <span class="n">msg_cache</span> <span class="o">=</span> <span class="p">{},</span> <span class="n">cf_cache</span> <span class="o">=</span> <span class="p">{},</span> <span class="n">gradient</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">patch_type</span> <span class="o">=</span> <span class="s1">&#39;zero&#39;</span><span class="p">):</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;Initializes the FINAL_ApproxNode instance.</span>
<span class="sd">		</span>
<span class="sd">		Args:</span>
<span class="sd">			model (HookedTransformer): </span>
<span class="sd">				The transformer model instance. It is assumed to be a HookedTransformer from transformer_lens library. Any other implementation which provide the same interface should work as well.</span>
<span class="sd">			layer (int): </span>
<span class="sd">				Layer index in the transformer. Embedding layer is assumed to be layer 0.</span>
<span class="sd">			metric (callable):</span>
<span class="sd">				A callable that takes as input a tensor of shape (batch_size, seq_len, d_model) and returns a scalar tensor.</span>
<span class="sd">				It is used to compute the gradient of the output with respect to the input of this node.</span>
<span class="sd">			position (int, default=None): </span>
<span class="sd">				Token position if position-specific, else None. None is equivalent to all positions.</span>
<span class="sd">			parent (ApproxNode, default=None): </span>
<span class="sd">				Parent node in the next node in the path. The parent is a successor in the computational graph.</span>
<span class="sd">			children (set, default=set()): </span>
<span class="sd">				Set of child nodes. A child is a predecessor in the computational graph.</span>
<span class="sd">			msg_cache (dict): </span>
<span class="sd">				Clean activation cache. Can be obtained by running the model with hooks using the clean prompt and converting the result to a dictionary. It must be a dictionary because it might be modified by adding new cached entries, corresponding to the outputs of subcomponents (e.g. single attention heads). </span>
<span class="sd">			cf_cache (dict, default={}): </span>
<span class="sd">				Counterfactual activation cache. Can be obtained by running the model with hooks using the corrupted prompt and converting the result to a dictionary. It must be a dictionary because it might be modified by adding new cached entries, corresponding to the outputs of subcomponents (e.g. single attention heads).</span>
<span class="sd">			gradient (Tensor, default=None): </span>
<span class="sd">				Node cached gradient. Usually is used to represent the gradient of the final output with respect to the input of this node, passing trough the path from final node to the current one.</span>
<span class="sd">			patch_type (str, default=&#39;zero&#39;): </span>
<span class="sd">				Type of intervention (&#39;zero&#39; or &#39;counterfactual&#39;). Zero patching corresponds to removing the message from the first node in the path to the input of the next node, while counterfactual patching corresponds to replacing the message with the counterfactual activation. In both cases the effect of the path is then calculated by propagating the message through the whole path.</span>
<span class="sd">		</span>
<span class="sd">		Returns:</span>
<span class="sd">			self (FINAL_ApproxNode):</span>
<span class="sd">				An instance of the FINAL_ApproxNode class.</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span> <span class="n">parent</span><span class="o">=</span><span class="n">parent</span><span class="p">,</span> <span class="n">children</span><span class="o">=</span><span class="n">children</span><span class="p">,</span> <span class="n">msg_cache</span><span class="o">=</span><span class="n">msg_cache</span><span class="p">,</span> <span class="n">cf_cache</span><span class="o">=</span><span class="n">cf_cache</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient</span><span class="p">,</span> <span class="n">input_name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;blocks.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">.hook_resid_post&quot;</span><span class="p">,</span> <span class="n">output_name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;blocks.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">.hook_resid_post&quot;</span><span class="p">,</span> <span class="n">patch_type</span><span class="o">=</span><span class="n">patch_type</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">metric</span> <span class="o">=</span> <span class="n">metric</span></div>

<div class="viewcode-block" id="FINAL_ApproxNode.forward">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.FINAL_ApproxNode.forward">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">message</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">		Calculate the effect of the message on the output of the node. </span>
<span class="sd">		</span>
<span class="sd">		The effect is calculated indirectly as the difference between the normal output of the component and the </span>
<span class="sd">		one obtained when the message is removed from the input of the node.</span>
<span class="sd">		On the other hand, if message is None the behavior depends on the patch_type:</span>
<span class="sd">		- &#39;zero&#39;: returns the normal output of the component</span>
<span class="sd">		- &#39;counterfactual&#39;: returns the difference between the normal output and the counterfactual output of the component</span>

<span class="sd">		Args:</span>

<span class="sd">			message (Tensor of shape (batch_size, seq_len, d_model), default=None):</span>
<span class="sd">				The message whose effect on the node need to be evaluated. If None, returns the normal </span>
<span class="sd">				output or the difference between normal and counterfactual output depending on patch_type.</span>
<span class="sd">			</span>
<span class="sd">		Returns:</span>
<span class="sd">			Tensor:</span>
<span class="sd">				A tensor representing the effect of the message on the output of the node.</span>
<span class="sd">				In simpler terms, it represents the message caused by passing the input message through this node.</span>

<span class="sd">		Notes:</span>
<span class="sd">			- If a position is specified the output will be zero for all other positions.</span>
<span class="sd">			- The method assumes that the msg_cache and cf_cache contain the necessary activations.</span>
<span class="sd">			- When message is None, the method will cache the output in msg_cache or cf_cache if not already present.</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">if</span> <span class="n">message</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
			<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_type</span> <span class="o">==</span> <span class="s1">&#39;zero&#39;</span><span class="p">:</span>
				<span class="n">res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
			<span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_type</span> <span class="o">==</span> <span class="s1">&#39;counterfactual&#39;</span><span class="p">:</span>
				<span class="n">res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">cf_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
			<span class="k">else</span><span class="p">:</span>
				<span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown patch type: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">patch_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
		<span class="k">else</span><span class="p">:</span>
			<span class="n">res</span> <span class="o">=</span> <span class="n">message</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
		<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
			<span class="n">res_zeroed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">res</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
			<span class="n">res_zeroed</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">res</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="p">:]</span>
			<span class="k">return</span> <span class="n">res_zeroed</span>
		<span class="k">return</span> <span class="n">res</span></div>


<div class="viewcode-block" id="FINAL_ApproxNode.calculate_gradient">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.FINAL_ApproxNode.calculate_gradient">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="nf">calculate_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_precomputed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">		Calculates the gradient of the node&#39;s input with respect to the final output.</span>
<span class="sd">		By default the gradient is calculated propagating backwards from the parent node if present,</span>
<span class="sd">		or assuming a gradient of ones if self has no parent. When &#39;grad_outputs&#39; is specified, it is used instead of the parent&#39;s gradient.</span>

<span class="sd">		Args:</span>
<span class="sd">			grad_outputs : Tensor, optional (default=None)</span>
<span class="sd">				Usually the gradient to propagate backwards in this particular case it is never used.</span>
<span class="sd">			</span>
<span class="sd">			save : bool, optional (default=True)</span>
<span class="sd">				Whether to save the computed gradient in self.gradient. The gradient can be reused</span>
<span class="sd">				later by setting use_precomputed to True.</span>
<span class="sd">			</span>
<span class="sd">			use_precomputed : bool, optional (default=False)</span>
<span class="sd">				Whether to use the precomputed gradient if available. The precoputed gradient is stored whenever</span>
<span class="sd">				save is True.</span>
<span class="sd">			</span>
<span class="sd">			metric : callable, optional (default=None)</span>
<span class="sd">				A callable that takes as input a tensor of shape (batch_size, seq_len, d_model) and returns a scalar tensor.</span>
<span class="sd">				It is used to compute the gradient of the output with respect to the input of this node.</span>
<span class="sd">				If None, uses the metric provided at initialization. If neither is provided, raises an error.</span>
<span class="sd">		</span>
<span class="sd">		Returns:</span>
<span class="sd">			gradient : Tensor</span>
<span class="sd">				A tensor representing the gradient of the output with respect to the input</span>
<span class="sd">				of this node, passing trough the path from final node to the current one.</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">use_precomputed</span><span class="p">:</span>
			<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
		<span class="k">if</span> <span class="n">metric</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
			<span class="n">metric</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">metric</span>
		<span class="k">if</span> <span class="n">metric</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
			<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;FINAL_ApproxNode.calculate_gradient() requires to provide a metric either at initialization or as a parameter&quot;</span><span class="p">)</span>
		<span class="n">input_residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">output_name</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
		<span class="n">input_residual</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
		<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
			<span class="n">output</span> <span class="o">=</span> <span class="n">metric</span><span class="p">(</span><span class="n">corrupted_resid</span><span class="o">=</span><span class="n">input_residual</span><span class="p">)</span>

		<span class="n">gradient</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span>
			<span class="n">output</span><span class="p">,</span>
			<span class="n">input_residual</span><span class="p">,</span>
			<span class="n">allow_unused</span><span class="o">=</span><span class="kc">True</span>
		<span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
		
		<span class="k">if</span> <span class="n">save</span><span class="p">:</span>
			<span class="bp">self</span><span class="o">.</span><span class="n">gradient</span> <span class="o">=</span> <span class="o">-</span><span class="n">gradient</span>
		<span class="k">return</span> <span class="o">-</span><span class="n">gradient</span></div>


<div class="viewcode-block" id="FINAL_ApproxNode.get_expansion_candidates">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.FINAL_ApproxNode.get_expansion_candidates">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="nf">get_expansion_candidates</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_cfg</span><span class="p">:</span> <span class="n">HookedTransformerConfig</span><span class="p">,</span> <span class="n">include_head</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">separate_kv</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">ApproxNode</span><span class="p">]:</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">		Returns the list of predecessors nodes in the computational graph whose outputs influence the output of this node. </span>
<span class="sd">		For the FINAL node, these are all MLP, EMBED and ATTN nodes from all layers.</span>

<span class="sd">		Args:</span>

<span class="sd">			model_cfg (HookedTransformerConfig):</span>
<span class="sd">				The configuration of the transformer model. It is used to determine the number of heads and other model parameters.</span>

<span class="sd">			include_head (bool, default=False):</span>
<span class="sd">				Whether to consider specific head nodes for ATTN.</span>

<span class="sd">			separate_kv (bool, default=False):</span>
<span class="sd">				Whether to consider key and value positions separately for ATTN nodes </span>
<span class="sd">		</span>
<span class="sd">		Returns:</span>
<span class="sd">			list of ApproxNode:</span>
<span class="sd">				The list of all nodes.</span>
<span class="sd">		</span>
<span class="sd">		Notes:</span>
<span class="sd">			- If self.position is None, only non-position-specific previous nodes are considered.</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="n">prev_nodes</span> <span class="o">=</span> <span class="p">[]</span>
		<span class="n">common_args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;msg_cache&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">msg_cache</span><span class="p">,</span> <span class="s2">&quot;parent&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;patch_type&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_type</span><span class="p">,</span> <span class="s2">&quot;cf_cache&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">cf_cache</span><span class="p">}</span>

		<span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model_cfg</span><span class="o">.</span><span class="n">n_layers</span><span class="p">):</span>
			<span class="c1"># MLPs</span>
			<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">MLP_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>

			<span class="c1"># ATTN query positions</span>
			<span class="k">if</span> <span class="n">include_head</span><span class="p">:</span>
				<span class="n">prev_nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span>  <span class="n">patch_key</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model_cfg</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)])</span>
			<span class="k">else</span><span class="p">:</span>
				<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span>  <span class="n">patch_key</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>

			<span class="c1"># ATTN key-value positions</span>
			<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
				<span class="k">for</span> <span class="n">keyvalue_position</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
					<span class="k">if</span> <span class="n">include_head</span><span class="p">:</span>
						<span class="k">if</span> <span class="n">separate_kv</span><span class="p">:</span>
							<span class="n">prev_nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model_cfg</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)])</span>
							<span class="n">prev_nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model_cfg</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)])</span>
						<span class="k">else</span><span class="p">:</span>
							<span class="n">prev_nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model_cfg</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)])</span>
					<span class="k">else</span><span class="p">:</span>
						<span class="k">if</span> <span class="n">separate_kv</span><span class="p">:</span>
							<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>
							<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>
						<span class="k">else</span><span class="p">:</span>
							<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="n">keyvalue_position</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>
			<span class="k">else</span><span class="p">:</span>
				<span class="k">if</span> <span class="n">include_head</span><span class="p">:</span>
					<span class="k">if</span> <span class="n">separate_kv</span><span class="p">:</span>
						<span class="n">prev_nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model_cfg</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)])</span>
						<span class="n">prev_nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model_cfg</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)])</span>
					<span class="k">else</span><span class="p">:</span>
						<span class="n">prev_nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model_cfg</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)])</span>
				<span class="k">else</span><span class="p">:</span>
					<span class="k">if</span> <span class="n">separate_kv</span><span class="p">:</span>
						<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>
						<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>
					<span class="k">else</span><span class="p">:</span>
						<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ATTN_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="n">keyvalue_position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">patch_key</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_value</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">patch_query</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>

		<span class="n">prev_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">EMBED_ApproxNode</span><span class="p">(</span><span class="n">layer</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="o">**</span><span class="n">common_args</span><span class="p">))</span>
		<span class="c1"># Remove duplicates</span>
		<span class="n">prev_nodes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">prev_nodes</span><span class="p">))</span>
		<span class="k">return</span> <span class="n">prev_nodes</span></div>


<div class="viewcode-block" id="FINAL_ApproxNode.__repr__">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.FINAL_ApproxNode.__repr__">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;Returns a string representation of the FINAL_ApproxNode instance.</span>
<span class="sd">		Includes layer and position if specified.</span>
<span class="sd">		Returns:</span>
<span class="sd">			str:</span>
<span class="sd">				A string representation of the FINAL_ApproxNode instance.</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="n">pos_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;, position=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
		<span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;FINAL_ApproxNode(layer=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="si">}{</span><span class="n">pos_str</span><span class="si">}</span><span class="s2">)&quot;</span></div>


<div class="viewcode-block" id="FINAL_ApproxNode.__hash__">
<a class="viewcode-back" href="../../../api/nodes.html#backward_search_approximated.utils.nodes.FINAL_ApproxNode.__hash__">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="fm">__hash__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;Hash function for the FINAL_ApproxNode instance.</span>
<span class="sd">		Returns:</span>
<span class="sd">			int:</span>
<span class="sd">				A hash value based on the layer and position.</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">return</span> <span class="nb">hash</span><span class="p">((</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">))</span> </div>
</div>

</pre></div>

                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2025, Andrea Cerutti. Released under the GNU General Public License v3.0.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.2.3.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>