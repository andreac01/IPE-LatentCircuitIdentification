{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d63b8fb",
   "metadata": {},
   "source": [
    "# Guide: **Understanding search methods to Isolate Path Effects**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8275da46",
   "metadata": {},
   "source": [
    "## Isolating Path Effect for Latent Circuit Identidentification\n",
    "\n",
    "\n",
    "**IPE (Isolating Path Effects)** is a novel method for **mechanistic interpretability** that identifies entire computational paths in large language models. \n",
    "Unlike common approaches that focus on individual edges, IPE uses an efficient top-down search to find paths from input embeddings to output logits that significantly affect model predictions. The method precisely measures a path's contribution by selectively modifying messages along the residual stream, allowing for both **path ablation** and **counterfactual replacement** while avoiding interference from other paths. \n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"./images/path.png\" alt=\"Isolating Path Effects\" width=\"250vh\"><br>\n",
    "    <text>Figure 1: An example of a path leading from the first token embedding to the residual associated with the final prediction </text>\n",
    "</p>\n",
    "\n",
    "\n",
    "At the core of the IPE code there are multiple distinct search strategies to efficiently find relevant computational paths. All strategies operate as a backward search, starting from the final model output and moving layer by layer back towards the input.\n",
    "\n",
    "## Shared Logic: \n",
    "\n",
    "Each strategy shares a common framework, which is the reason we collect all these search methods under the umbrella of IPE:\n",
    "\n",
    " - Path Representation: Paths are represented as a list of Node objects, with each Node representing a component (e.g., attention heads, MLPs, Embedding) at a particular layer and position. Attention nodes may correspond to single heads and gather information from a residual in a previous position (key-value position)\n",
    "\n",
    " - Path Evaluation: The evaluation function is the most critical component. It takes a complete or partial path and calculates its contribution to the model's output using a user-defined metric. This is where the core \"path isolation\" logic happens, modifying the residual stream messages.\n",
    "\n",
    " - Backward Search: All metrics evaluates how the removal of the contribution of a path from the final residual would affect the model prediction. The subsequent need to evaluate up to the final node makes approaching the search starting from the final residual down to the input natural.\n",
    "\n",
    " - Pruning: The search space is pruned based on the calculated contribution. Paths that don't meet a certain criterion (threshold, top-n candidates, BFS) are discarded to maintain computational efficiency. Note that the number of space is exponential in the number of layer, with base proportional to the number of heads times the number of position, therefore it is unfeasible to perform an exhaustive search.\n",
    "\n",
    "## Differenes:\n",
    "\n",
    "We present two different types of algorithm differentiated by the path evaluation modality, where the second method uses a linear approximation of the first:\n",
    " -Path Message Patching\n",
    " - Path Attribution Patching\n",
    "\n",
    "Furthermore, as previously highlighted, different pruning methodologies yields slightly different search approaches (threshold, top-n candidates, BFS).\n",
    "In conclusion 6 different search function to Isolate Path Effect are present in the IPE library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aee347",
   "metadata": {},
   "source": [
    "## **Path Message Patching**\n",
    "\n",
    "Path Message Patching is the core method for isolating and quantifying the effect of a specific path. It works by performing a modified forward pass along the selected path, note that this forward pass involves only the nodes in the path, rather than the entire model. Instead of simply zeroing out an activation, which would affect all downstream paths, this method precisely isolates the contribution of the targeted path.\n",
    "\n",
    "Before discussing path message patching, it is necessary to clarify some concepts: path, message, and node input.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <div style=\"display:flex; gap:1.5rem; justify-content:center; align-items:flex-start; flex-wrap:wrap;\">\n",
    "        <figure style=\"text-align:center; margin:0;\">\n",
    "            <img src=\"./images/path.png\" alt=\"Isolating Path Effects\" style=\"width:12rem; height:auto; max-width:40vw;\">\n",
    "            <figcaption style=\"font-size:0.9rem; margin-top:0.25rem;\">(a)</figcaption>\n",
    "        </figure>\n",
    "        <figure style=\"text-align:center; margin:0;\">\n",
    "            <img src=\"./images/inputs.png\" alt=\"Inputs\" style=\"width:12rem; height:auto; max-width:40vw;\">\n",
    "            <figcaption style=\"font-size:0.9rem; margin-top:0.25rem;\">(b)</figcaption>\n",
    "        </figure>\n",
    "        <figure style=\"text-align:center; margin:0;\">\n",
    "            <img src=\"./images/path_message.png\" alt=\"Inputs\" style=\"width:12rem; height:auto; max-width:40vw;\">\n",
    "            <figcaption style=\"font-size:0.9rem; margin-top:0.25rem;\">(c)</figcaption>\n",
    "        </figure>\n",
    "    </div>\n",
    "    <text>Figure 2: (a) An example of a path leading from the first token embedding to the residual associated with the final prediction. (b) Visualization of a node input as the sum of the contributions from all its predecessors. (c) Visualization of the path message calculation as the effect of propagating an ablation through the path (note that X can be either zeros for ablation or a value for the counterfactual).</text>\n",
    "</p>\n",
    "\n",
    "### Key Definitions\n",
    "\n",
    "Some important definitions for understanding the method:\n",
    "\n",
    " - **Node:** A node is a model component. Node types include embeddings, attention heads, MLPs, and the final residual. Each node is characterized by the position where its output is written and the layer it belongs to. For attention, nodes can be further specified by the key-value position and the specific head.\n",
    "\n",
    " - **Node Input:** The input to a node is the residual stream value at that point in the forward pass.\n",
    "\n",
    " - **Message:** The message from node A to node B is the portion of B's input attributable to the output of node A. A node's input is the sum of all its predecessors' outputs.\n",
    "\n",
    " - **Path:** A path is a sequence of consecutive nodes in the computational tree representing the model.\n",
    "\n",
    "### Path Message\n",
    "\n",
    "Message calculation is performed through an iterative, bottom-up procedure (from the first node in the path to the last):\n",
    "\n",
    " 1. **Initial Message Removal:** Starting with the first node in the path, compute the change (∆$m_i$) as the difference between the output from the first node under the counterfactual input (or zero, for ablation) and the output under the original input.\n",
    "\n",
    " 2. **Iterative Propagation:** Next, compute the variation in the message of the subsequent component caused by patching the previous node. The next variation (∆$m_{i+1}$) is calculated as the difference in the output of $node_{n+1}$ when its input is varied by adding ∆$m_i$, compared to the output under the original input.\n",
    "\n",
    " 3. **Path Message:** The variation in the model's final residual attributable to the path is the message variation obtained for the last path node.\n",
    "\n",
    "This approach enables both path ablation (setting the path's contribution to zero) and path counterfactual replacement (replacing the path's effect with what would have been produced by a different input), simply by changing the initial message removal method.\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "Once the path message is computed, various metrics can be used to evaluate the effect of removing it. For example, the `target_logit_percentage` metric scores a path using the formula $Path\\_Score = (logit(unembed(resid)) - logit(unembed(resid - ∆m_{path})))/logit(resid)$, where logit is the logit score of the target token and unembed performs the final layernorm and unembedding.\n",
    "\n",
    "It is important to note that all metrics are defined by removing a message from the final residual stream. Consequently, a path must terminate at the final residual node to produce a contribution score. The path's start, however, can be anywhere (it does not have to be an embedding), so partial paths beginning mid-model can be evaluated. Paths that stop before the final residual node cannot be fully evaluated. This is why a top-down search approach is used.\n",
    "\n",
    "### **Path Message Patching with Threshold**\n",
    "\n",
    "The baseline algorithm for finding the most relevant paths is as follows:\n",
    "\n",
    "1. Start with a frontier containing a path of length zero, composed only of the `FINAL_Node`, a dummy node representing the final residual stream.\n",
    "2. For each path in the frontier, evaluate all possible continuations by extending the path with a new node.\n",
    "3. For each expansion whose contribution exceeds a specified threshold (optionally using the absolute value), either add it to the frontier if it is incomplete, or add it to the list of relevant complete paths.\n",
    "4. Repeat step 2 until the frontier is empty.\n",
    "\n",
    "The actual implementation includes two further approximations for evaluating expansions involving attention:\n",
    "\n",
    "1. **batch_positions:** Evaluate the relevance of individual key-value positions only after confirming that altering all positions simultaneously is relevant.\n",
    "2. **batch_heads:** Evaluate the relevance of individual heads only after confirming that the contribution of the entire attention module is relevant.\n",
    "\n",
    "Below is an example of how to invoke the path message patching with threshold function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4082f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "(total 1)    Frontier: [(tensor(91.0508, grad_fn=<MeanBackward0>), [FINAL_Node(layer=11, position=14)])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(total 79)    Frontier: [(tensor(14.3535, grad_fn=<MeanBackward0>), [ATTN_Node(layer=9, head=9, position=14, keyvalue_position=None, patch_query=True, patch_key=False, patch_value=False), FINAL_Node(layer=11, position=14)]), (tensor(14.1150, grad_fn=<MeanBackward0>), [ATTN_Node(layer=9, head=9, position=14, keyvalue_position=4, patch_query=False, patch_key=True, patch_value=True), FINAL_Node(layer=11, position=14)])]... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:29<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(total 707)    Frontier: [(tensor(28.7930, grad_fn=<MeanBackward0>), [ATTN_Node(layer=11, head=0, position=14, keyvalue_position=None, patch_query=True, patch_key=False, patch_value=False), MLP_Node(layer=11, position=14), FINAL_Node(layer=11, position=14)]), (tensor(10.4667, grad_fn=<MeanBackward0>), [ATTN_Node(layer=11, head=8, position=14, keyvalue_position=None, patch_query=True, patch_key=False, patch_value=False), MLP_Node(layer=11, position=14), FINAL_Node(layer=11, position=14)])]... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 707/707 [02:13<00:00,  5.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(total 1002)    Frontier: [(tensor(5.9460, grad_fn=<MeanBackward0>), [ATTN_Node(layer=0, head=9, position=11, keyvalue_position=None, patch_query=True, patch_key=False, patch_value=False), MLP_Node(layer=0, position=11), ATTN_Node(layer=11, head=8, position=14, keyvalue_position=11, patch_query=False, patch_key=True, patch_value=True), FINAL_Node(layer=11, position=14)]), (tensor(5.3043, grad_fn=<MeanBackward0>), [ATTN_Node(layer=0, head=9, position=4, keyvalue_position=None, patch_query=True, patch_key=False, patch_value=False), MLP_Node(layer=0, position=4), ATTN_Node(layer=11, head=10, position=14, keyvalue_position=4, patch_query=False, patch_key=True, patch_value=True), FINAL_Node(layer=11, position=14)])]... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1002/1002 [03:17<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(total 722)    Frontier: [(tensor(7.2668, grad_fn=<MeanBackward0>), [ATTN_Node(layer=0, head=11, position=11, keyvalue_position=None, patch_query=True, patch_key=False, patch_value=False), MLP_Node(layer=0, position=11), MLP_Node(layer=1, position=11), ATTN_Node(layer=11, head=8, position=14, keyvalue_position=11, patch_query=False, patch_key=True, patch_value=True), FINAL_Node(layer=11, position=14)]), (tensor(4.9825, grad_fn=<MeanBackward0>), [MLP_Node(layer=0, position=5), ATTN_Node(layer=8, head=3, position=14, keyvalue_position=5, patch_query=False, patch_key=True, patch_value=True), MLP_Node(layer=8, position=14), ATTN_Node(layer=9, head=6, position=14, keyvalue_position=None, patch_query=True, patch_key=False, patch_value=False), FINAL_Node(layer=11, position=14)])]... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 722/722 [01:46<00:00,  6.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(total 264)    Frontier: [(tensor(2.7075, grad_fn=<MeanBackward0>), [MLP_Node(layer=0, position=3), MLP_Node(layer=3, position=3), ATTN_Node(layer=5, head=5, position=10, keyvalue_position=3, patch_query=False, patch_key=True, patch_value=True), ATTN_Node(layer=8, head=6, position=14, keyvalue_position=10, patch_query=False, patch_key=True, patch_value=True), ATTN_Node(layer=9, head=9, position=14, keyvalue_position=None, patch_query=True, patch_key=False, patch_value=False), FINAL_Node(layer=11, position=14)]), (tensor(2.5997, grad_fn=<MeanBackward0>), [ATTN_Node(layer=0, head=9, position=11, keyvalue_position=None, patch_query=True, patch_key=False, patch_value=False), MLP_Node(layer=0, position=11), MLP_Node(layer=1, position=11), ATTN_Node(layer=11, head=8, position=14, keyvalue_position=11, patch_query=False, patch_key=True, patch_value=True), MLP_Node(layer=11, position=14), FINAL_Node(layer=11, position=14)])]... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 264/264 [00:30<00:00,  8.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(total 75)    Frontier: [(tensor(1.4843, grad_fn=<MeanBackward0>), [ATTN_Node(layer=0, head=11, position=10, keyvalue_position=None, patch_query=True, patch_key=False, patch_value=False), MLP_Node(layer=0, position=10), MLP_Node(layer=2, position=10), ATTN_Node(layer=6, head=9, position=10, keyvalue_position=None, patch_query=True, patch_key=False, patch_value=False), ATTN_Node(layer=8, head=6, position=14, keyvalue_position=10, patch_query=False, patch_key=True, patch_value=True), ATTN_Node(layer=9, head=9, position=14, keyvalue_position=None, patch_query=True, patch_key=False, patch_value=False), FINAL_Node(layer=11, position=14)]), (tensor(1.4621, grad_fn=<MeanBackward0>), [ATTN_Node(layer=3, head=0, position=10, keyvalue_position=None, patch_query=True, patch_key=False, patch_value=False), MLP_Node(layer=3, position=10), MLP_Node(layer=4, position=10), ATTN_Node(layer=6, head=9, position=10, keyvalue_position=None, patch_query=True, patch_key=False, patch_value=False), ATTN_Node(layer=8, head=6, position=14, keyvalue_position=10, patch_query=False, patch_key=True, patch_value=True), ATTN_Node(layer=9, head=9, position=14, keyvalue_position=None, patch_query=True, patch_key=False, patch_value=False), FINAL_Node(layer=11, position=14)])]... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [00:04<00:00, 15.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(total 16)    Frontier: [(tensor(1.3035, grad_fn=<MeanBackward0>), [MLP_Node(layer=2, position=10), ATTN_Node(layer=3, head=0, position=10, keyvalue_position=None, patch_query=True, patch_key=False, patch_value=False), MLP_Node(layer=3, position=10), MLP_Node(layer=4, position=10), ATTN_Node(layer=6, head=9, position=10, keyvalue_position=None, patch_query=True, patch_key=False, patch_value=False), ATTN_Node(layer=8, head=6, position=14, keyvalue_position=10, patch_query=False, patch_key=True, patch_value=True), ATTN_Node(layer=9, head=9, position=14, keyvalue_position=None, patch_query=True, patch_key=False, patch_value=False), FINAL_Node(layer=11, position=14)]), (tensor(0.9832, grad_fn=<MeanBackward0>), [ATTN_Node(layer=0, head=9, position=2, keyvalue_position=0, patch_query=False, patch_key=True, patch_value=True), ATTN_Node(layer=3, head=0, position=10, keyvalue_position=2, patch_query=False, patch_key=True, patch_value=True), MLP_Node(layer=3, position=10), MLP_Node(layer=4, position=10), ATTN_Node(layer=6, head=9, position=10, keyvalue_position=None, patch_query=True, patch_key=False, patch_value=False), ATTN_Node(layer=8, head=6, position=14, keyvalue_position=10, patch_query=False, patch_key=True, patch_value=True), ATTN_Node(layer=9, head=9, position=14, keyvalue_position=None, patch_query=True, patch_key=False, patch_value=False), FINAL_Node(layer=11, position=14)])]... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:02<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(total 1)    Frontier: [(tensor(0.5339, grad_fn=<MeanBackward0>), [MLP_Node(layer=0, position=10), MLP_Node(layer=2, position=10), ATTN_Node(layer=3, head=0, position=10, keyvalue_position=None, patch_query=True, patch_key=False, patch_value=False), MLP_Node(layer=3, position=10), MLP_Node(layer=4, position=10), ATTN_Node(layer=6, head=9, position=10, keyvalue_position=None, patch_query=True, patch_key=False, patch_value=False), ATTN_Node(layer=8, head=6, position=14, keyvalue_position=10, patch_query=False, patch_key=True, patch_value=True), ATTN_Node(layer=9, head=9, position=14, keyvalue_position=None, patch_query=True, patch_key=False, patch_value=False), FINAL_Node(layer=11, position=14)])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(total 2)    Frontier: [(tensor(1.1124, grad_fn=<MeanBackward0>), [ATTN_Node(layer=0, head=9, position=10, keyvalue_position=None, patch_query=True, patch_key=False, patch_value=False), MLP_Node(layer=0, position=10), MLP_Node(layer=2, position=10), ATTN_Node(layer=3, head=0, position=10, keyvalue_position=None, patch_query=True, patch_key=False, patch_value=False), MLP_Node(layer=3, position=10), MLP_Node(layer=4, position=10), ATTN_Node(layer=6, head=9, position=10, keyvalue_position=None, patch_query=True, patch_key=False, patch_value=False), ATTN_Node(layer=8, head=6, position=14, keyvalue_position=10, patch_query=False, patch_key=True, patch_value=True), ATTN_Node(layer=9, head=9, position=14, keyvalue_position=None, patch_query=True, patch_key=False, patch_value=False), FINAL_Node(layer=11, position=14)]), (tensor(0.6922, grad_fn=<MeanBackward0>), [ATTN_Node(layer=0, head=11, position=10, keyvalue_position=None, patch_query=True, patch_key=False, patch_value=False), MLP_Node(layer=0, position=10), MLP_Node(layer=2, position=10), ATTN_Node(layer=3, head=0, position=10, keyvalue_position=None, patch_query=True, patch_key=False, patch_value=False), MLP_Node(layer=3, position=10), MLP_Node(layer=4, position=10), ATTN_Node(layer=6, head=9, position=10, keyvalue_position=None, patch_query=True, patch_key=False, patch_value=False), ATTN_Node(layer=8, head=6, position=14, keyvalue_position=10, patch_query=False, patch_key=True, patch_value=True), ATTN_Node(layer=9, head=9, position=14, keyvalue_position=None, patch_query=True, patch_key=False, patch_value=False), FINAL_Node(layer=11, position=14)])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 35.46it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "from ipe.metrics import target_logit_percentage\n",
    "from ipe.graph_search import  PathMessagePatching\n",
    "from ipe.nodes import FINAL_Node\n",
    "from functools import partial\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\")\n",
    "_, cache = model.run_with_cache(\"When John and Mary went to the shop, John gave the bags to\")\n",
    "target = \" Mary\"\n",
    "metric = partial(target_logit_percentage, model=model, clean_final_resid=cache['blocks.11.hook_resid_post'], target_tokens=[model.to_single_token(target)])\n",
    "\n",
    "paths =  PathMessagePatching(\n",
    "    model=model,\n",
    "    metric=metric,\n",
    "    root=FINAL_Node(\n",
    "        model=model,\n",
    "        metric=metric,\n",
    "        layer=11,\n",
    "        position=cache['blocks.11.hook_resid_post'].shape[1] - 1,\n",
    "        msg_cache=dict(cache), # Message cache, it must be a dictionary\n",
    "        cf_cache={}, # Counterfactual cache, if you want to use a different cf input than the clean one\n",
    "        patch_type='zero' # Patch type, can be 'zero' or 'counterfactual', if 'counterfactual' cf_cache must be provided\n",
    "    ),\n",
    "    min_contribution = 0.5, # Minimum contribution for a path to be considered\n",
    "    include_negative = True, # Wether to include paths with negative contribution\n",
    "    return_all = False, # Wether to return all paths found or only the highly contributing ones\n",
    "    batch_positions = True, # Wether to first evaluate the effect of patching all positions at one, then one by one\n",
    "    batch_heads = True # Wether to first evaluate the effect of patching all heads at one, then one by one\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "844a613c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 388 paths with contribution > 0.5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {len(paths)} paths with contribution > 0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cf6ed4",
   "metadata": {},
   "source": [
    "### **Path Message Patching with Best First Search**\n",
    "\n",
    "The **Best First Search** (BFS) style of search for finding the most relevant paths operates with a slightly different prioritization than a threshold-based approach. Instead of exploring all paths that meet a minimum contribution, it focuses on the most promising paths at each step.\n",
    "\n",
    "1.  Start with a **priority queue** containing a path of length zero, composed only of the `FINAL_Node`. The priority of this initial path is set to a high value (e.g., the full model's logit difference).\n",
    "2.  While the priority queue is not empty, pop the path with the highest current contribution.\n",
    "3.  Evaluate all possible continuations by extending the path with a new node.\n",
    "4.  For each new expansion, calculate its contribution and push it onto the priority queue.\n",
    "5.  If a path reaches the `EMBEDDING_Node`, it is considered a complete path and is added to the list of relevant paths.\n",
    "6.  The search continues until the priority queue is empty or a predefined number of paths have been found.\n",
    "\n",
    "Like the threshold-based approach, this method also incorporates approximations to handle the complexity of attention modules, specifically **batching positions** and **batching heads** to efficiently identify promising candidates before evaluating individual components. The key difference is that BFS guarantees that the paths with the highest scores are explored first, making it well-suited for finding the top-k most important paths without needing to set a specific threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9794e3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Completed paths:  60%|██████    | 181/300 [10:00<06:34,  3.32s/it]\n"
     ]
    }
   ],
   "source": [
    "from ipe.graph_search import PathMessagePatching_BestFirstSearch\n",
    "paths =  PathMessagePatching_BestFirstSearch(\n",
    "    model=model,\n",
    "    metric=metric,\n",
    "    root=FINAL_Node(\n",
    "        model=model,\n",
    "        metric=metric,\n",
    "        layer=11,\n",
    "        position=cache['blocks.11.hook_resid_post'].shape[1] - 1,\n",
    "        msg_cache=dict(cache), # Message cache, it must be a dictionary\n",
    "        cf_cache={}, # Counterfactual cache, if you want to use a different cf input than the clean one\n",
    "        patch_type='zero' # Patch type, can be 'zero' or 'counterfactual', if 'counterfactual' cf_cache must be provided\n",
    "    ),\n",
    "    top_n=300, # Number of nodes to find\n",
    "    max_time=600, # Maximum time in seconds to run the search\n",
    "    include_negative = True, # Wether to include paths with negative contribution\n",
    "    batch_positions = True, # Wether to first evaluate the effect of patching all positions at one, then one by one\n",
    "    batch_heads = True # Wether to first evaluate the effect of patching all heads at one, then one by one\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08ab0d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 181 relevant paths\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {len(paths)} relevant paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b84dc6",
   "metadata": {},
   "source": [
    "### **Path Message Patching with Limited Level Width**\n",
    "\n",
    "The Limited Level Width approach is based on a Breadth First Search, similarly to the thresholding approach. However, instead of adding to the frontier only nodes achieving a minimum contribution, the top k candidate incomplete paths are added to the frontier.\n",
    "\n",
    "1. Start with a frontier containing a path of length zero, composed only of the `FINAL_Node`, a dummy node representing the final residual stream.\n",
    "2. For each path in the frontier, evaluate all possible continuations by extending the path with a new node.\n",
    "3. Whenever a path is completed add it to the list of completed paths.\n",
    "4. Once all path in the frontier has been expanded keep the k ones having the highest contribution that are not completed.\n",
    "5. Repeat from step 2 until the frontier is empty.\n",
    "\n",
    "The actual implementation includes two further approximations for evaluating expansions involving attention:\n",
    "\n",
    "1. **batch_positions:** Evaluate the relevance of individual key-value positions only after confirming that altering all positions simultaneously is relevant.\n",
    "2. **batch_heads:** Evaluate the relevance of individual heads only after confirming that the contribution of the entire attention module is relevant.\n",
    "\n",
    "Below is an example of how to invoke the path message patching with threshold function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfb2ab86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding level (size 1): 100%|██████████| 1/1 [00:00<00:00, 41.92it/s]\n",
      "Expanding level (size 200): 100%|██████████| 200/200 [00:19<00:00, 10.06it/s]\n",
      "Expanding level (size 200): 100%|██████████| 200/200 [00:30<00:00,  6.60it/s]\n",
      "Expanding level (size 200): 100%|██████████| 200/200 [00:21<00:00,  9.40it/s]\n",
      "Expanding level (size 200): 100%|██████████| 200/200 [00:21<00:00,  9.50it/s]\n",
      "Expanding level (size 200): 100%|██████████| 200/200 [00:13<00:00, 15.25it/s]\n",
      "Expanding level (size 200): 100%|██████████| 200/200 [00:08<00:00, 23.18it/s]\n",
      "Expanding level (size 200): 100%|██████████| 200/200 [00:07<00:00, 25.90it/s]\n",
      "Expanding level (size 200): 100%|██████████| 200/200 [00:05<00:00, 35.99it/s]\n",
      "Expanding level (size 200): 100%|██████████| 200/200 [00:05<00:00, 37.21it/s]\n",
      "Expanding level (size 200): 100%|██████████| 200/200 [00:04<00:00, 42.09it/s]\n",
      "Expanding level (size 200): 100%|██████████| 200/200 [00:04<00:00, 43.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from ipe.graph_search import PathMessagePatching_LimitedLevelWidth\n",
    "paths =  PathMessagePatching_LimitedLevelWidth(\n",
    "    model=model,\n",
    "    metric=metric,\n",
    "    root=FINAL_Node(\n",
    "        model=model,\n",
    "        metric=metric,\n",
    "        layer=11,\n",
    "        position=cache['blocks.11.hook_resid_post'].shape[1] - 1,\n",
    "        msg_cache=dict(cache), # Message cache, it must be a dictionary\n",
    "        cf_cache={}, # Counterfactual cache, if you want to use a different cf input than the clean one\n",
    "        patch_type='zero' # Patch type, can be 'zero' or 'counterfactual', if 'counterfactual' cf_cache must be provided\n",
    "    ),\n",
    "    max_width=200, # Maximum number of nodes to expand at each level\n",
    "    include_negative = True, # Wether to include paths with negative contribution\n",
    "    batch_positions = True, # Wether to first evaluate the effect of patching all positions at one, then one by one\n",
    "    batch_heads = True # Wether to first evaluate the effect of patching all heads at one, then one by one\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8cfb74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2201 paths\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {len(paths)} paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8015f42",
   "metadata": {},
   "source": [
    "## **Path Attribution Patching**\n",
    "\n",
    "\n",
    "Path Attribution Patching is a **linear approximation** of the more computationally intensive **Path Message Patching**. This method leverages the **chain rule**, a core principle of backpropagation, to efficiently estimate a path's contribution to the final output. The key insight is that the gradient of the loss with respect to a specific activation can be decomposed into the sum of contributions from all computational paths leading from that activation to the final loss.\n",
    "\n",
    "Instead of performing a full modified forward pass, Path Attribution Patching approximates a path's contribution as the product of the change in an edge's activation and the gradient of the final path output with respect to that edge. \n",
    "It is important to underline how the gradient along the path output is different from the gradient of the output with respect to the activation. To briefly explain the difference we can look at Figure 3. \n",
    "In the image we can see the computational graph of a simple function *F* of *x*:\n",
    "$$\n",
    "    F(x) = f(g(x) + h(x) + x)\n",
    "$$\n",
    "The gradient of the output *F(x)* with respect to *x* is the following:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial F}{\\partial x} = \\frac{\\partial F(x)}{\\partial g(x)} \\frac{\\partial g(x)}{\\partial x} + \\frac{\\partial F(x)}{\\partial h(x)} \\frac{\\partial h(x)}{\\partial x} + \\frac{\\partial F(x)}{\\partial x}\n",
    "$$\n",
    "\n",
    "We want to focus our attention of how this gradient is divided into three different terms, each corresponding to a path leading the input *x* up to the final node output. We refer to these as **gradients along the path**\n",
    "In general it holds that:\n",
    "The gradient of the output with respect to an intermediate activation is the sum of all gradients along each different path leading the activation to the final output:\n",
    "$$\n",
    "\\frac{\\partial F}{\\partial x} = \\sum_{path \\in \\text{paths }} \\left( \\prod_{node \\in path} \\frac{\\partial out_{node}}{\\partial in_{node}} \\right)\n",
    "$$\n",
    "*where each term in the sum corresponds to the gradient along a specific computational path $p$ from $x$ to the output.*\n",
    "\n",
    "Note that the gradient along a single path can be computed by the product of the gradients $\\frac{\\partial out_i}{\\partal in_i} of the output of a componet $out_i$ with respect to its input $in_i$ for each node in the path (due to the backpropagation rule).\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "    <div style=\"display:flex; gap:1.5rem; justify-content:center; align-items:flex-start; flex-wrap:wrap;\">\n",
    "        <figure style=\"text-align:center; margin:0;\">\n",
    "            <img src=\"./images/path.png\" alt=\"Isolating Path Effects\" style=\"width:12rem; height:auto; max-width:40vw;\">\n",
    "            <figcaption style=\"font-size:0.9rem; margin-top:0.25rem;\">(a)</figcaption>\n",
    "        </figure>\n",
    "        <figure style=\"text-align:center; margin:0;\">\n",
    "            <img src=\"./images/inputs.png\" alt=\"Inputs\" style=\"width:12rem; height:auto; max-width:40vw;\">\n",
    "            <figcaption style=\"font-size:0.9rem; margin-top:0.25rem;\">(b)</figcaption>\n",
    "        </figure>\n",
    "        <figure style=\"text-align:center; margin:0;\">\n",
    "            <img src=\"./images/path_message.png\" alt=\"Inputs\" style=\"width:12rem; height:auto; max-width:40vw;\">\n",
    "            <figcaption style=\"font-size:0.9rem; margin-top:0.25rem;\">(c)</figcaption>\n",
    "        </figure>\n",
    "    </div>\n",
    "    <text>Figure 2: (a) An example of a path leading from the first token embedding to the residual associated with the final prediction. (b) Visualization of a node input as the sum of the contributions from all its predecessors. (c) Visualization of the path message calculation as the effect of propagating an ablation through the path (note that X can be either zeros for ablation or a value for the counterfactual).</text>\n",
    "</p>\n",
    "\n",
    "\n",
    "### Path Evaluation via Attribution\n",
    "\n",
    "Given a path it's **Attribution Score** is computed as follows:\n",
    "\n",
    " 1. **Initial Message Change **: Starting with the first node in the path, compute the change (∆$m_{initial}$) as the difference between the output from the first node under the counterfactual input (or zero, for ablation) and the output under the original input. Note that these values can be easily cached by saving the activation of the clean and corrupted forward passes.\n",
    "\n",
    "    $$ \\Delta m^{initial} = in^{initial}_{corr} - in^{initial}_{clean} $$\n",
    "    where $in^{initial}$ is the input associated with the first node int the path\n",
    "\n",
    " 2. **Gradient Along the Path**: Compute the gradient of the output with respect to the path initial message as:\n",
    "\n",
    "    $$\\nabla _{path} out = \\frac{\\partial out}{\\partial path} = \\frac{\\partial out_{path}}{\\partial path} = \\prod_{node \\in path} \\frac{\\partial out_{node}}{\\partial in_{node}}$$\n",
    "\n",
    "    Note that an equivalent formula applies if we are analyzing a metric $L$ which is a function in the model output:\n",
    "\n",
    "    $$\\nabla _{path} L(out) = \\frac{\\partial L(out)}{\\partial path} = \\frac{\\partial L(out)}{\\partial out} \\cdot \\prod_{node \\in path} \\frac{\\partial out_{node}}{\\partial in_{node}}$$\n",
    "\n",
    "    Note that the computation of the gradient along a path obtained by adding a node ($n0$) to an incomplete path ($path$) is relatively fast if we already have computed $\\nabla _p L(out)$, it just require a backward pass on the single component $n0$:\n",
    "\n",
    "    $$\\nabla _{(candidate \\cup path)} L(out) =  \\frac{\\partial L(out)}{\\partial path} \\cdot  \\frac{\\partial out_{n0}}{\\partial in_{n0}} $$\n",
    "\n",
    " 3. **Path Attribution Score**: The final score for a path is approximated by the dot product of the cached initial message change and the path gradient. This can be expressed using a first order Taylor expansion as:\n",
    "\n",
    "    $$L(out_{clean} | Do(path_{clean} = path_{corr})) \\approx L(out_{clean}) + (\\Delta m_{initial})^T \\nabla _{path} L(out) $$\n",
    "\n",
    "\n",
    "### Efficiency gain\n",
    "\n",
    "At a first glance this method of evaluating a path is not faster than directly computing the value of $L(out_{clean} | Do(path_{clean} = path_{corr})) - L(out_{clean})$. This is true when we have to evaluate a single path, because the approximated method requires a custom backward pass along the path which is computantionally slightly more expensive than computing the forward pass along the path.\n",
    "\n",
    "Nonetheless, during our backward search, when we want to expand an incomplete path, we need to calculate the effect of thousands of possible continuations of it. \n",
    "When using Path Message Passing this leads to a new forward pass along each candidate path.\n",
    "On the other hand the evaluation of all possible expansions using Path Attribution Patching is much faster. \n",
    "Particularly the initial message is actually obtainable as the sum of all the activations on the incoming edges, therefore by isolating them we can compute an attribution score for each of the possible path extension.\n",
    "\n",
    "$$ \\Delta m^{initial} = in^{initial}_{corr} - in^{initial}_{clean} = \\sum_{candidate \\in predecessors} out^{candidate}_{corr} - \\sum_{candidate \\in predecessors} out^{candidate}_{clean} $$\n",
    "\n",
    "So isolating the message due to a contribution of the edge coming from a single candidate child:\n",
    "\n",
    "$$ \\Delta m^{from candiate} = out^{candidate}_{corr} - out^{candidate}_{clean} $$\n",
    "\n",
    "The product between this variation in the input message and the gradient along the path yields the score of the path attributable to the edge linking to the candidate.\n",
    "\n",
    "$$L(out_{clean} | Do((candidate \\cup path)_{clean} = (candidate \\cup path)_{corr})) \\approx L(out_{clean}) + (\\Delta m_{from candidate})^T \\nabla _{path} L(out) $$\n",
    "\n",
    "After calculating the gradient along the path, the cost of calculating the attribution score of each new contribution is just the one of computing a dot product (note that the edge messages can be easily cached in just two initial forward passes).\n",
    "\n",
    "\n",
    "### Implementation\n",
    "\n",
    "As for Path Message Patching the mechanism for evaluating the path allows for the same backward search methodologies. However due to the speed of evaluation of candidate expansion there is no need for further heuristics such as first evaluating the contribution from all positions and/or all heads before addressing the contribution of a single component.\n",
    "\n",
    "We want to highlight how once again the search starting from the final node, moving backward to the initial nodes is more computationally efficient because the output can gradient computation is a vector $\\cdot$ matrix multiplication in this case, while when going forward it would require matrix $\\cdot$ matrix multiplications. This is simply caused by the fact that the gradient $\\frac{\\partial L(out)}{\\partial out}$ is a vector, while for a generic node $\\frac{\\partial node_{out}}{\\partial node_{in}}$ is a Jacobian matrix (of size d_model x d_model, so unfeasible to be cached)\n",
    "\n",
    "\n",
    "\n",
    "### **Path Attribution Patching with Threshold**\n",
    "\n",
    "The baseline algorithm for finding the most relevant paths is equivalent to the corresponding Path Message Patching implementation:\n",
    "\n",
    "1. Start with a frontier containing a path of length zero, composed only of the `FINAL_Node`, a dummy node representing the final residual stream.\n",
    "2. For each path in the frontier, evaluate all possible continuations by extending the path with a new node.\n",
    "3. For each expansion whose contribution exceeds a specified threshold (optionally using the absolute value), either add it to the frontier if it is incomplete, or add it to the list of relevant complete paths.\n",
    "4. Repeat step 2 until the frontier is empty.\n",
    "\n",
    "Below is an example of how to invoke the path message patching with threshold function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a84e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:12<00:00, 12.77s/it]\n",
      "100%|██████████| 104/104 [00:10<00:00,  9.59it/s]\n",
      "100%|██████████| 1151/1151 [00:58<00:00, 19.61it/s]\n",
      "100%|██████████| 2585/2585 [01:19<00:00, 32.61it/s] \n",
      "100%|██████████| 2373/2373 [00:40<00:00, 58.81it/s] \n",
      "100%|██████████| 1715/1715 [00:17<00:00, 99.63it/s] \n",
      "100%|██████████| 870/870 [00:06<00:00, 130.43it/s]\n",
      "100%|██████████| 168/168 [00:01<00:00, 127.59it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 233.78it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "from ipe.metrics import target_logit_percentage\n",
    "from ipe.graph_search import  PathAttributionPatching\n",
    "from ipe.nodes import FINAL_Node\n",
    "from functools import partial\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\")\n",
    "_, cache = model.run_with_cache(\"When John and Mary went to the shop, John gave the bags to\")\n",
    "target = \" Mary\"\n",
    "metric = partial(target_logit_percentage, model=model, clean_final_resid=cache['blocks.11.hook_resid_post'], target_tokens=[model.to_single_token(target)])\n",
    "\n",
    "minimum_contribution_threshold = 2\n",
    "\n",
    "paths =  PathAttributionPatching(\n",
    "    model=model,\n",
    "    metric=metric,\n",
    "    root=FINAL_Node(\n",
    "        model=model,\n",
    "        metric=metric,\n",
    "        layer=11,\n",
    "        position=cache['blocks.11.hook_resid_post'].shape[1] - 1,\n",
    "        msg_cache=dict(cache), # Message cache, it must be a dictionary\n",
    "        cf_cache={}, # Counterfactual cache, if you want to use a different cf input than the clean one\n",
    "        patch_type='zero' # Patch type, can be 'zero' or 'counterfactual', if 'counterfactual' cf_cache must be provided\n",
    "    ),\n",
    "    min_contribution = 0.5, # Minimum contribution for a path to be considered\n",
    "    include_negative = True, # Wether to include paths with negative contribution\n",
    "    confirm_relevance = False, # If true this became an hybrid approach, attribution selects relevant candidate expansions, then message patching confirms them before adding them to the path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f409c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 990 paths\n",
      "Found 537 paths with contribution > 0.5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {len(paths)} paths\")\n",
    "actually_important_paths = [(c, p) for c, p in paths if abs(c) > 0.5]\n",
    "print(f\"Found {len(actually_important_paths)} paths with contribution > 0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4124d26c",
   "metadata": {},
   "source": [
    "### **Path Attribution Patching with Best First Search**\n",
    "\n",
    "The **Best First Search** (BFS) style of search for finding the most relevant paths operates with a slightly different prioritization than a threshold-based approach. Instead of exploring all paths that meet a minimum contribution, it focuses on the most promising paths at each step.\n",
    "\n",
    "1.  Start with a **priority queue** containing a path of length zero, composed only of the `FINAL_Node`. The priority of this initial path is set to a high value (e.g., the full model's logit difference).\n",
    "2.  While the priority queue is not empty, pop the path with the highest current contribution.\n",
    "3.  Evaluate all possible continuations by extending the path with a new node.\n",
    "4.  For each new expansion, calculate its contribution and push it onto the priority queue.\n",
    "5.  If a path reaches the `EMBEDDING_Node`, it is considered a complete path and is added to the list of relevant paths.\n",
    "6.  The search continues until the priority queue is empty or a predefined number of paths have been found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2195d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipe.graph_search import  PathAttributionPatching_BestFirstSearch\n",
    "\n",
    "\n",
    "paths =  PathAttributionPatching_BestFirstSearch(\n",
    "    model=model,\n",
    "    metric=metric,\n",
    "    root=FINAL_Node(\n",
    "        model=model,\n",
    "        metric=metric,\n",
    "        layer=11,\n",
    "        position=cache['blocks.11.hook_resid_post'].shape[1] - 1,\n",
    "        msg_cache=dict(cache), # Message cache, it must be a dictionary\n",
    "        cf_cache={}, # Counterfactual cache, if you want to use a different cf input than the clean one\n",
    "        patch_type='zero' # Patch type, can be 'zero' or 'counterfactual', if 'counterfactual' cf_cache must be provided\n",
    "    ),\n",
    "    top_n = 100, # Minimum contribution for a path to be considered\n",
    "    max_time = 900, # Maximum time in seconds to run the search\n",
    "    include_negative = True, # Wether to include paths with negative contribution\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d1c749",
   "metadata": {},
   "source": [
    "### **Path Attribution Patching with Limited Level Width**\n",
    "\n",
    "The Limited Level Width approach is based on a Breadth First Search, similarly to the thresholding approach. However, instead of adding to the frontier only nodes achieving a minimum contribution, the top k candidate incomplete paths are added to the frontier.\n",
    "\n",
    "1. Start with a frontier containing a path of length zero, composed only of the `FINAL_Node`, a dummy node representing the final residual stream.\n",
    "2. For each path in the frontier, evaluate all possible continuations by extending the path with a new node.\n",
    "3. Whenever a path is completed add it to the list of completed paths.\n",
    "4. Once all path in the frontier has been expanded keep the k ones having the highest contribution that are not completed.\n",
    "5. Repeat from step 2 until the frontier is empty.\n",
    "\n",
    "Below is an example of how to invoke the path message patching with threshold function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59290ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipe.graph_search import  PathAttributionPatching_LimitedLevelWidth\n",
    "\n",
    "\n",
    "paths =  PathAttributionPatching_LimitedLevelWidth(\n",
    "    model=model,\n",
    "    metric=metric,\n",
    "    root=FINAL_Node(\n",
    "        model=model,\n",
    "        metric=metric,\n",
    "        layer=11,\n",
    "        position=cache['blocks.11.hook_resid_post'].shape[1] - 1,\n",
    "        msg_cache=dict(cache), # Message cache, it must be a dictionary\n",
    "        cf_cache={}, # Counterfactual cache, if you want to use a different cf input than the clean one\n",
    "        patch_type='zero' # Patch type, can be 'zero' or 'counterfactual', if 'counterfactual' cf_cache must be provided\n",
    "    ),\n",
    "    max_width = 100, # Maximum width of the search\n",
    "    include_negative = True, # Wether to include paths with negative contribution\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
